{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26337f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from neurovlm.retrieval_resources import (\n",
    "    _load_dataframe, _load_latent_text\n",
    ")\n",
    "from neurovlm.data import data_dir\n",
    "from neurovlm.train import Trainer, which_device\n",
    "from neurovlm.models import ConceptClf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d567237d",
   "metadata": {},
   "source": [
    "# Concept Classifier\n",
    "\n",
    "The concept classifier predicts which concepts are present given a latent neuro embeddings. The top-10 related concepts are passed to an LLM to summarize the brain map. Here, Llama-3.1-8B-Instruct is used to generated interpretations. Any language model may be used. Larger models or models trained one neuroscience literature may provided better brain map interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d36ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram embeddings, from 06_n_grams.ipyn\n",
    "ngram_emb = torch.load(data_dir / \"ngram_emb.pt\")\n",
    "mask = np.load(data_dir / \"ngram_mask.npy\")\n",
    "\n",
    "# load text\n",
    "df = _load_dataframe()\n",
    "text = df[\"name\"] + \" [SEP] \" + df[\"description\"]\n",
    "\n",
    "# load pre-computed ngrams from 06_n_grams.ipynb\n",
    "X = np.load(data_dir / \"ngram_matrix.npy\")[:, mask]\n",
    "features = np.load(data_dir / \"ngram_labels.npy\")[mask]\n",
    "\n",
    "# load latent text\n",
    "latent, pmids = _load_latent_text()\n",
    "\n",
    "# align df order, on pmid\n",
    "inds = df[\"pmid\"].argsort().values\n",
    "df = df.iloc[inds]\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "X = X[inds]\n",
    "assert (df['pmid'] == pmids).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8f031ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity as target\n",
    "y = latent @ (ngram_emb / ngram_emb.norm(dim=1)[:, None]).T\n",
    "m = (y < 0.) | (torch.from_numpy(X) == 0.)\n",
    "y[m==1] = 0.\n",
    "\n",
    "# transform cosine similarity ~= probabilities\n",
    "t = 0.03\n",
    "tau = 0.08\n",
    "y = torch.sigmoid((y - t)/ tau)\n",
    "\n",
    "y[m] = 0.\n",
    "y = y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab2b403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure latent neuro vectors align with df\n",
    "latent_neuro, pmid = torch.load(\n",
    "    data_dir / \"latent_neuro.pt\", weights_only=False, map_location=\"cpu\"\n",
    ").values()\n",
    "\n",
    "assert (df[\"pmid\"] == df[\"pmid\"].sort_values()).all()\n",
    "\n",
    "mask = df['pmid'].isin(pmid)\n",
    "df, y = df[mask], y[mask]\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe037598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data splits\n",
    "train_ids, test_ids, val_ids = torch.load(data_dir / \"pmids_split.pt\", weights_only=False).values()\n",
    "train_ids.sort()\n",
    "val_ids.sort()\n",
    "test_ids.sort()\n",
    "\n",
    "def split(df, latent, y, pmids, device):\n",
    "    mask = df['pmid'].isin(pmids).to_numpy()\n",
    "    X = latent[torch.from_numpy(mask)].clone().to(device)\n",
    "    y = torch.from_numpy(y[mask].copy()).float().to(device)\n",
    "    pmids = pmids[pd.Series(pmids).isin(df[\"pmid\"])]\n",
    "    return X, y, pmids\n",
    "\n",
    "device = which_device()\n",
    "X_train, y_train, train_ids = split(df, latent_neuro, y, train_ids, device)\n",
    "X_val, y_val, val_ids = split(df, latent_neuro, y,  val_ids, device)\n",
    "X_test, y_test, test_ids = split(df, latent_neuro, y, test_ids, device)\n",
    "\n",
    "# ensure sorted\n",
    "assert (df['pmid'] == df['pmid'].sort_values()).all()\n",
    "assert (train_ids == np.sort(train_ids)).all()\n",
    "assert (val_ids == np.sort(val_ids)).all()\n",
    "assert (test_ids == np.sort(test_ids)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d837b4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: -1, val loss: 0.71944\n",
      "Epoch: 0, val loss: 0.1559\n",
      "Epoch: 20, val loss: 0.054673\n",
      "Epoch: 40, val loss: 0.054531\n",
      "Epoch: 60, val loss: 0.054543\n",
      "Epoch: 80, val loss: 0.0546\n",
      "Epoch: 100, val loss: 0.054684\n",
      "Epoch: 120, val loss: 0.054806\n",
      "Epoch: 140, val loss: 0.054949\n",
      "Epoch: 160, val loss: 0.055094\n",
      "Epoch: 180, val loss: 0.055262\n"
     ]
    }
   ],
   "source": [
    "clf = ConceptClf(X.shape[1]).to(device)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "trainer = Trainer(\n",
    "    clf,\n",
    "    loss_fn,\n",
    "    lr=5e-5,\n",
    "    n_epochs=200,\n",
    "    batch_size=1028,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    interval=20\n",
    ")\n",
    "\n",
    "trainer.fit(X_train, y_train)\n",
    "\n",
    "trainer.save(data_dir / \"concept_clf.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
