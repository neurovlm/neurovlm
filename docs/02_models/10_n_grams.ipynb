{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26337f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from neurovlm.retrieval_resources import (\n",
    "    _load_dataframe, _load_latent_text\n",
    ")\n",
    "from neurovlm.data import data_dir\n",
    "from neurovlm.train import Trainer, which_device\n",
    "from neurovlm.models import ConceptClf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d567237d",
   "metadata": {},
   "source": [
    "# Concept Classifier\n",
    "\n",
    "The concept classifier predicts which concepts are present given a latent neuro embeddings. The top-10 related concepts are passed to an LLM to summarize the brain map. Here, Llama-3.1-8B-Instruct is used to generated interpretations. Any language model may be used. Larger models or models trained one neuroscience literature may provided better brain map interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d36ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram embeddings, from 06_n_grams.ipyn\n",
    "ngram_emb = torch.load(data_dir / \"ngram_emb.pt\")\n",
    "\n",
    "# load text\n",
    "df = _load_dataframe()\n",
    "df.sort_values(by=\"pmid\", inplace=True)\n",
    "text = df[\"name\"] + \" [SEP] \" + df[\"description\"]\n",
    "\n",
    "# load pre-computed ngrams from 06_n_grams.ipynb\n",
    "X = np.load(data_dir / \"ngram_matrix.npy\")\n",
    "features = np.load(data_dir / \"ngram_labels.npy\")\n",
    "\n",
    "# load latent text\n",
    "latent, pmids = _load_latent_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8f031ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity as target\n",
    "y = latent @ (ngram_emb / ngram_emb.norm(dim=1)[:, None]).T\n",
    "m = (y < 0.) | (torch.from_numpy(X) == 0.)\n",
    "y[m==1] = 0.\n",
    "\n",
    "# transform cosine similarity ~= probabilities\n",
    "t = 0.03\n",
    "tau = 0.08\n",
    "y = torch.sigmoid((y - t)/ tau)\n",
    "\n",
    "y[m] = 0.\n",
    "y = y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab2b403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure latent neuro vectors align with df\n",
    "latent_neuro, pmid = torch.load(\n",
    "    data_dir / \"latent_neuro.pt\", weights_only=False, map_location=\"cpu\"\n",
    ").values()\n",
    "\n",
    "assert (df[\"pmid\"] == df[\"pmid\"].sort_values()).all()\n",
    "\n",
    "mask = df['pmid'].isin(pmid)\n",
    "df, y = df[mask], y[mask]\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe037598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data splits\n",
    "train_ids, test_ids, val_ids = torch.load(data_dir / \"pmids_split.pt\", weights_only=False).values()\n",
    "train_ids.sort()\n",
    "val_ids.sort()\n",
    "test_ids.sort()\n",
    "\n",
    "def split(df, latent, y, pmids, device):\n",
    "    mask = df['pmid'].isin(pmids).to_numpy()\n",
    "    X = latent[torch.from_numpy(mask)].clone().to(device)\n",
    "    y = torch.from_numpy(y[mask].copy()).float().to(device)\n",
    "    pmids = pmids[pd.Series(pmids).isin(df[\"pmid\"])]\n",
    "    return X, y, pmids\n",
    "\n",
    "device = which_device()\n",
    "X_train, y_train, train_ids = split(df, latent_neuro, y, train_ids, device)\n",
    "X_val, y_val, val_ids = split(df, latent_neuro, y,  val_ids, device)\n",
    "X_test, y_test, test_ids = split(df, latent_neuro, y, test_ids, device)\n",
    "\n",
    "# ensure sorted\n",
    "assert (df['pmid'] == df['pmid'].sort_values()).all()\n",
    "assert (train_ids == np.sort(train_ids)).all()\n",
    "assert (val_ids == np.sort(val_ids)).all()\n",
    "assert (test_ids == np.sort(test_ids)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d837b4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: -1, val loss: 0.69668\n",
      "Epoch: 0, val loss: 0.5344\n",
      "Epoch: 20, val loss: 0.043564\n",
      "Epoch: 40, val loss: 0.042261\n",
      "Epoch: 60, val loss: 0.041339\n",
      "Epoch: 80, val loss: 0.040851\n",
      "Epoch: 100, val loss: 0.040587\n",
      "Epoch: 120, val loss: 0.040443\n",
      "Epoch: 140, val loss: 0.040348\n",
      "Epoch: 160, val loss: 0.040287\n",
      "Epoch: 180, val loss: 0.040233\n",
      "Epoch: 200, val loss: 0.040211\n"
     ]
    }
   ],
   "source": [
    "clf = ConceptClf(X.shape[1]).to(device)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "trainer = Trainer(\n",
    "    clf,\n",
    "    loss_fn,\n",
    "    lr=3e-5,\n",
    "    n_epochs=201,\n",
    "    batch_size=1028,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    interval=20,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "trainer.fit(X_train, y_train)\n",
    "\n",
    "trainer.save(data_dir / \"concept_clf.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75671b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurovlm.retrieval_resources import _load_masker, _load_autoencoder\n",
    "import gzip, pickle\n",
    "import nibabel as nib\n",
    "from nilearn.image import resample_to_img\n",
    "\n",
    "masker = _load_masker()\n",
    "autoencoder = _load_autoencoder()\n",
    "\n",
    "# Load network atlases\n",
    "with gzip.open(data_dir / \"networks_arrays.pkl.gz\", \"rb\") as f:\n",
    "    networks = pickle.load(f)\n",
    "    \n",
    "networks = [(k, n, nib.Nifti1Image(networks[k][n][\"array\"], affine=networks[k][n][\"affine\"]))\n",
    "            for k in networks.keys() for n in networks[k].keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c26b8624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top predicted terms for Du_VIS-P:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['visual', 'cuneus', 'right lingual gyrus', 'early visual cortex',\n",
       "       'age', 'memory', 'precuneus', 'primary visual cortex', 'early',\n",
       "       'relative healthy controls', 'visual network', 'blind', 'motor',\n",
       "       'faces', 'cortical areas', 'field', 'visual areas', 'left right',\n",
       "       'occipital lobe', 'lateral occipital cortex', 'occipital regions',\n",
       "       'face', 'active', 'vision', 'cognitive impairment amci', 'dorsal',\n",
       "       'right cuneus', 'component analysis ica', 'maps', 'global',\n",
       "       'future', 'robust', 'eye', 'scene', 'self',\n",
       "       'low frequency fluctuation', 'goal', 'left lingual gyrus',\n",
       "       'anterior posterior cingulate', 'gray matter density', 'aging',\n",
       "       'level dependent signal', 'group independent component', 'multi',\n",
       "       'adult', 'visual stimuli', 'default mode network',\n",
       "       'posterior parietal cortex', 'seen', 'superior parietal lobe'],\n",
       "      dtype='<U40')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "x = masker.transform(\n",
    "    resample_to_img(networks[i][2], masker.mask_img, interpolation=\"nearest\", force_resample=True, copy_header=True)\n",
    ")\n",
    "x = autoencoder.encoder(torch.from_numpy(x))\n",
    "\n",
    "scores = torch.sigmoid(clf(x.to(\"cuda\")).cpu().detach())\n",
    "print(f\"top predicted terms for {networks[i][0] + \"_\" + networks[i][1]}:\")\n",
    "features[scores.argsort(descending=True)][:50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
