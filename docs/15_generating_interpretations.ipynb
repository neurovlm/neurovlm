{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26337f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle, gzip\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# from keybert import KeyBERT\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nilearn.image import resample_to_img\n",
    "from nilearn.plotting import view_img\n",
    "\n",
    "from neurovlm.retrieval_resources import (\n",
    "    _load_dataframe, _load_specter, _load_latent_text,\n",
    "    _load_masker, _load_autoencoder, _load_networks\n",
    ")\n",
    "from neurovlm.data import data_dir\n",
    "from neurovlm.train import Trainer, which_device\n",
    "# from neurovlm.models import ConceptClf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d567237d",
   "metadata": {},
   "source": [
    "# Interpreting Brain Maps\n",
    "\n",
    "## Corpus Extraction\n",
    "Extract n-grams for the training corpus. N-grams are weighted by cosine similarity to article embeddings, e.g. if n-gram is highly similar to the articles it gets a value near 1, otherwise it gets a value near 0.\n",
    "\n",
    "# Concept Classifier\n",
    "The concept classifier predicts which concepts are present given a latent neuro embeddings. The top-10 related concepts are passed to an LLM to summarize the brain map. Here, Llama-3.1-8B-Instruct is used to generated interpretations. Any language model may be used. Larger models or models trained one neuroscience literature may provided better brain map interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a4519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(docs, ngram_range):\n",
    "    counts = CountVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        stop_words=\"english\",\n",
    "        min_df=1\n",
    "    ).fit(docs)\n",
    "\n",
    "    X = counts.transform(docs)  # shape: (n_docs, n_features)\n",
    "\n",
    "    feature_names = counts.get_feature_names_out()\n",
    "\n",
    "    mask = np.array(X.sum(axis=0) >= 100)[0]\n",
    "\n",
    "    X = np.array(X[:, mask].todense())\n",
    "    feature_names = feature_names[mask]\n",
    "\n",
    "    return X, feature_names\n",
    "\n",
    "# load text\n",
    "df = _load_dataframe()\n",
    "text = df[\"name\"] + \" [SEP] \" + df[\"description\"]\n",
    "\n",
    "# extract n-grams\n",
    "if not (data_dir / \"ngram_matrix.npy\").exists():\n",
    "    X_uni, features_uni = extract_ngrams(text, (1, 1))\n",
    "    X_bi, features_bi = extract_ngrams(text, (2, 2))\n",
    "    X_tri, features_tri = extract_ngrams(text, (3, 3))\n",
    "    X = np.hstack((X_uni, X_bi, X_tri))\n",
    "    features = np.concat((features_uni, features_bi, features_tri))\n",
    "    np.save(data_dir / \"ngram_matrix.npy\", X)\n",
    "    np.save(data_dir / \"ngram_labels.npy\", features.astype(str))\n",
    "else:\n",
    "    # load pre-computed\n",
    "    X = np.load(data_dir / \"ngram_matrix.npy\")\n",
    "    features = np.load(data_dir / \"ngram_labels.npy\")\n",
    "    \n",
    "# manual cleaning\n",
    "DROP_SUBSTRINGS = [\n",
    "    # study-like language\n",
    "    \"study\", \"studies\", \"result\", \"indicate\", \"show\", \"related\",\n",
    "    \"differences\", \"significant\", \"effect\", \"role\", \"measure\",\n",
    "    \"displayed\", \"involved\", \"examined\", \"associated\", \"altered\",\n",
    "    \"performed\", \"demonstrated\", \"conclus\", \"correlate\", \"individuals\",\n",
    "    \"common\", \"prior\",\n",
    "    # too general\n",
    "    \"brain\", \"neural\", \"neuroimaging\", \"mri\", \"fmri\", \"connectivity\",\n",
    "    \"diagnosed\", \"patients\", \"little\", \"known\", \"activation\", \"blood\",\n",
    "    \"alterations\", \"neuroscience\", \"people\", # \"magnetic\"\n",
    "]\n",
    "\n",
    "DROP_REGEXES = [\n",
    "    r\"^cortex\",\n",
    "    # general single terms\n",
    "    r\"^ventral$\", r\"^frontal$\", r\"^neuronal$\", r\"^cognitive$\",\n",
    "    r\"^cerebral$\", r\"^resting_state$\",  r\"^disorder$\",\n",
    "    r\"^neuropsychological$\", r\"^cognition$\", r\"^stimulus$\",\n",
    "    r\"^dysfunction$\", r\"^imaging$\", r\"^functional$\",\n",
    "    r\"^functional imaging$\", r\"^task performance$\", r\"^impairments$\",\n",
    "    r\"^traits$\", r\"^dysfunction$\",  r\"^cognitive abilities$\", r\"^imaging dti$\",\n",
    "    # [SEP] token\n",
    "    r\"\\bsep\\b\",\n",
    "]\n",
    "\n",
    "pattern = \"|\".join(\n",
    "    [re.escape(s) for s in DROP_SUBSTRINGS] +  # plain terms\n",
    "    DROP_REGEXES                               # regex terms\n",
    ")\n",
    "\n",
    "mask = ~pd.Series(features).str.contains(pattern, case=False, na=False, regex=True)\n",
    "features = features[mask]\n",
    "X = X[:, mask]\n",
    "\n",
    "# load latent text\n",
    "latent, pmids = _load_latent_text()\n",
    "\n",
    "# align df order, on pmid\n",
    "inds = df[\"pmid\"].argsort().values\n",
    "df = df.iloc[inds]\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "X = X[inds]\n",
    "assert (df['pmid'] == pmids).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ec2261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specter embeddings for ngrams\n",
    "specter = _load_specter()\n",
    "specter.specter = specter.specter.eval()\n",
    "\n",
    "if not (data_dir / \"ngram_emb.pt\").exists():\n",
    "    ngram_emb = []\n",
    "    batch_size = 512\n",
    "    for i in tqdm(range(0, len(features), batch_size), total=ceil(len(features)//batch_size)):\n",
    "        with torch.no_grad():\n",
    "            ngram_emb.append(specter(features[i:i+batch_size].tolist()))\n",
    "    ngram_emb = torch.vstack(ngram_emb)\n",
    "\n",
    "    ngram_emb = ngram_emb / ngram_emb.norm(dim=1)[:, None] # unit vector\n",
    "    torch.save(ngram_emb, data_dir / \"ngram_emb.pt\")\n",
    "else:\n",
    "    ngram_emb = torch.load(data_dir / \"ngram_emb.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f031ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity as target\n",
    "y = latent @ (ngram_emb / ngram_emb.norm(dim=1)[:, None]).T\n",
    "m = (y < 0.) | (torch.from_numpy(X) == 0.)\n",
    "y[m==1] = 0.\n",
    "\n",
    "# transform cosine similarity ~= probabilities\n",
    "t = 0.03\n",
    "tau = 0.08\n",
    "y = torch.sigmoid((y - t)/ tau)\n",
    "\n",
    "y[m] = 0.\n",
    "y = y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2b403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure latent neuro vectors align with df\n",
    "latent_neuro, pmid = torch.load(\n",
    "    data_dir / \"latent_neuro_sparse.pt\", weights_only=False, map_location=\"cpu\"\n",
    ").values()\n",
    "\n",
    "assert (df[\"pmid\"] == df[\"pmid\"].sort_values()).all()\n",
    "\n",
    "mask = df['pmid'].isin(pmid)\n",
    "df, y = df[mask], y[mask]\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe037598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data splits\n",
    "train_ids, test_ids, val_ids = torch.load(data_dir / \"pmids_split.pt\", weights_only=False).values()\n",
    "train_ids.sort()\n",
    "val_ids.sort()\n",
    "test_ids.sort()\n",
    "\n",
    "def split(df, latent, y, pmids, device):\n",
    "    mask = df['pmid'].isin(pmids).to_numpy()\n",
    "    X = latent[torch.from_numpy(mask)].clone().to(device)\n",
    "    y = torch.from_numpy(y[mask].copy()).float().to(device)\n",
    "    pmids = pmids[pd.Series(pmids).isin(df[\"pmid\"])]\n",
    "    return X, y, pmids\n",
    "\n",
    "device = which_device()\n",
    "X_train, y_train, train_ids = split(df, latent_neuro, y, train_ids, device)\n",
    "X_val, y_val, val_ids = split(df, latent_neuro, y,  val_ids, device)\n",
    "X_test, y_test, test_ids = split(df, latent_neuro, y, test_ids, device)\n",
    "\n",
    "# ensure sorted\n",
    "assert (df['pmid'] == df['pmid'].sort_values()).all()\n",
    "assert (train_ids == np.sort(train_ids)).all()\n",
    "assert (val_ids == np.sort(val_ids)).all()\n",
    "assert (test_ids == np.sort(test_ids)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d837b4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: -1, val loss: 0.69402\n",
      "Epoch: 0, val loss: 0.48088\n",
      "Epoch: 20, val loss: 0.03878\n",
      "Epoch: 40, val loss: 0.037303\n",
      "Epoch: 60, val loss: 0.036246\n",
      "Epoch: 80, val loss: 0.035679\n",
      "Epoch: 100, val loss: 0.035344\n",
      "Epoch: 120, val loss: 0.035209\n",
      "Epoch: 140, val loss: 0.035075\n",
      "Epoch: 160, val loss: 0.035017\n",
      "Epoch: 180, val loss: 0.034976\n"
     ]
    }
   ],
   "source": [
    "class ConceptClf(nn.Module):\n",
    "    \"\"\"Predict concepts from latent neuro embeddings.\"\"\"\n",
    "    def __init__(self, d_out):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(384, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 1526),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1526, d_out)\n",
    "        )\n",
    "    def forward(self, X: torch.tensor):\n",
    "        return self.seq(X)\n",
    "    \n",
    "clf = ConceptClf(X.shape[1]).to(device)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "trainer = Trainer(\n",
    "    clf,\n",
    "    loss_fn,\n",
    "    lr=5e-5,\n",
    "    n_epochs=200,\n",
    "    batch_size=1028,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    interval=20\n",
    ")\n",
    "\n",
    "trainer.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d55d9d",
   "metadata": {},
   "source": [
    "## Network Correspondence\n",
    "\n",
    "Test geneartion on the network dataset. Predicted concepts are passed to the LLM to summarizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe3c16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "masker = _load_masker()\n",
    "autoencoder = _load_autoencoder()\n",
    "networks = _load_networks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab0c4ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Load LLM\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Eval mode and disable gradients\n",
    "model.eval()\n",
    "for p in model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0db8dbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a neuroscience editor writing a short wiki-style article from a list of terms.\n",
    "\n",
    "INPUT: a list of neuroscience terms (networks, brain regions, cognitive functions, disorders).\n",
    "OUTPUT: ONE article that uses the terms to form a coherent theme.\n",
    "\n",
    "Rules:\n",
    "1) Title (required): 6–12 words. Make it specific and content-based.\n",
    "   - Use 1–2 of the most informative terms (prefer: network/circuit + region + cognition; add disorder only if strongly supported).\n",
    "   - DO NOT use generic titles like: \"Summary\", \"Overview\", \"Brain Network Analysis\", \"A Summary of Terms\".\n",
    "\n",
    "2) Lead paragraph (2–3 sentences):\n",
    "   - State the unifying theme directly (what the terms collectively describe).\n",
    "   - Name 3–5 “anchor” terms that drive the theme.\n",
    "   - Do NOT say “the provided list”, “top-ranked”, “these terms appear”, or anything about scoring/ranking.\n",
    "\n",
    "3) Body sections:\n",
    "   - Networks\n",
    "   - Key Regions\n",
    "   - Cognitive Functions\n",
    "   - Clinical Relevance\n",
    "\n",
    "4) Be concrete:\n",
    "   - Prefer specific mechanisms, pathways, and canonical associations over vague statements.\n",
    "   - If a term is too vague/ambiguous/unrelated, ignore it in the main text.\n",
    "\n",
    "No references. Do not mention this prompt.\n",
    "\"\"\".strip()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6657d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "with gzip.open(data_dir / \"networks_arrays.pkl.gz\", \"rb\") as f:\n",
    "        networks = pickle.load(f)\n",
    "\n",
    "network_imgs = []\n",
    "for k in networks.keys():\n",
    "    for a in networks[k].keys():\n",
    "        network_imgs.append((k, a, nib.Nifti1Image(networks[k][a][\"array\"], affine=networks[k][a][\"affine\"])))\n",
    "\n",
    "networks = [i for i in network_imgs if i[0] not in [\"UKBICA\", \"HCPICA\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a58ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000b6c0356f2498c98402d9ded8f1154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode networks\n",
    "networks_emb = torch.zeros((len(networks), 384))\n",
    "for i, row in tqdm(enumerate(networks), total=len(networks)):\n",
    "    \n",
    "    # Encode network image\n",
    "    with torch.no_grad():\n",
    "        x = masker.transform(\n",
    "            resample_to_img(row[2], masker.mask_img, interpolation=\"nearest\", force_resample=True, copy_header=True)\n",
    "        )\n",
    "        x = autoencoder.encoder(torch.from_numpy(x))\n",
    "        networks_emb[i] = x\n",
    "\n",
    "torch.save(networks_emb, \"networks_emb.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "70870c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c342882b5494051815a9c72a046b2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = []\n",
    "\n",
    "for i, row in tqdm(enumerate(networks), total=len(networks)):\n",
    "    \n",
    "    # Concept classifier\n",
    "    scores = torch.sigmoid(clf(networks_emb[i].to(device)).cpu().detach())\n",
    "\n",
    "    # Pass concepts to LLM\n",
    "    user_prompt = \", \".join(features[scores.argsort().flip(0).numpy()[:20]])\n",
    "\n",
    "    messages.append([\n",
    "        {\"role\": \"system\", \"content\": system_prompt.strip(\"\\n\")},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eea72d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0313e969b80840b0be182b77f59a10a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_summaries = []\n",
    "\n",
    "batch_size = 5\n",
    "for i in tqdm(range(0, len(networks), batch_size), total=ceil(len(networks)/batch_size)):\n",
    "    with torch.inference_mode():\n",
    "        outputs = pipe(\n",
    "            messages[i:i+batch_size],\n",
    "            max_new_tokens=1000,\n",
    "            do_sample=True,\n",
    "            temperature=0.2,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.1,\n",
    "            return_full_text=True,\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "    for idx in range(len(outputs)):\n",
    "        generated_summary = outputs[idx][0][\"generated_text\"][-1][\"content\"].strip()\n",
    "        row = networks[i:i+batch_size][idx]\n",
    "        with open(f\"generated_summaries_{row[0].lower().replace(\"/\", \"-\")}_{row[1].lower().replace(\"/\", \"-\")}.txt\", \"w\") as f:\n",
    "            f.write(generated_summary)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nilearn import datasets, maskers\n",
    "\n",
    "atlas0 = datasets.fetch_atlas_harvard_oxford(\"cort-maxprob-thr25-2mm\")\n",
    "    \n",
    "atlas1 = datasets.fetch_atlas_schaefer_2018(\n",
    "    n_rois=400, yeo_networks=7, resolution_mm=2\n",
    ")\n",
    "\n",
    "labels0 = []\n",
    "labels1 = []\n",
    "for row in tqdm(networks, total=len(networks)):\n",
    "    for atlas, labels in [(atlas0, labels0), (atlas1, labels1)]:\n",
    "        img = row[-1]\n",
    "        labels_img = atlas.maps\n",
    "        _labels = list(atlas.labels)\n",
    "        masker = maskers.NiftiLabelsMasker(labels_img=labels_img, standardize=False)\n",
    "        region_means = masker.fit_transform(img)\n",
    "        df = pd.DataFrame({\n",
    "            \"label\": _labels[1:],\n",
    "            \"mean_activation\": region_means,\n",
    "        })\n",
    "        df = df.iloc[1:].sort_values(\"mean_activation\", ascending=False)\n",
    "        top_k = 5\n",
    "        top_regions = df.head(top_k)\n",
    "        labels.append(top_regions[\"label\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f824796",
   "metadata": {},
   "source": [
    "## PubMed Test Set\n",
    "todo: add test set results here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val_pred = trainer.model(X_val) # <- change to test set, not val\n",
    "\n",
    "# # ii = 1 # 6\n",
    "# # print(df[df['pmid'].isin(val_ids).to_numpy()].iloc[ii][\"name\"])\n",
    "# # print(df[df['pmid'].isin(val_ids).to_numpy()].iloc[ii][\"description\"])\n",
    "\n",
    "# # true\n",
    "# _df = pd.DataFrame({\n",
    "#     \"phrase\": features,\n",
    "#     \"score\": y_val[ii].to(\"cpu\").detach().numpy()\n",
    "# })\n",
    "\n",
    "# _df.sort_values(by=\"score\", ascending=False)\n",
    "\n",
    "# # predicted\n",
    "# _df = pd.DataFrame({\n",
    "#     \"phrase\": features,\n",
    "#     \"score\": torch.sigmoid(y_val_pred[ii].to(\"cpu\").detach()).numpy()\n",
    "# })\n",
    "# _df.sort_values(by=\"score\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
