{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f2e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import hashlib\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from neurovlm.data import data_dir, load_dataset\n",
    "from neurovlm.models import Specter\n",
    "from neurovlm.train import which_device\n",
    "device = which_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c1735a",
   "metadata": {},
   "source": [
    "# Text Encoding\n",
    "\n",
    "This notebook encodes (title, abstract) pairs using Specter. Specter was trained on scientific (title, abstract) pairs, suggesting it is likely to perform well with medium length queries. MiniLM-L6 is expected to better handle short form queries. \n",
    "\n",
    "\n",
    "Use specter is used to to encode (title, abstract) pairs to a 768 dimensional space.\n",
    "\n",
    "> A. Singh, M. D'Arcy, A. Cohan, D. Downey, and S. Feldman, “SciRepEval: A Multi-Format Benchmark for Scientific Document Representations,” in Proc. Conf. Empirical Methods in Natural Language Processing (EMNLP), 2022. [Online]. Available: https://api.semanticscholar.org/CorpusID:254018137\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c125f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There are adapters available but none are activated for the forward pass.\n"
     ]
    }
   ],
   "source": [
    "# Load publications dataframe\n",
    "df_pubs = load_dataset(\"publications\")\n",
    "\n",
    "# Check existing\n",
    "batch_size = 16\n",
    "overwrite = False\n",
    "suffix = \"\"\n",
    "\n",
    "if not overwrite and (data_dir / \"latent_specter2_adhoc.pt\").exists():\n",
    "    # Append to existing results\n",
    "    #   save compute when adding more papers\n",
    "    latent_text_adhoc_exist, pmids_text_adhoc_exist = torch.load(data_dir / \"latent_specter2_adhoc.pt\", weights_only=False).values()\n",
    "\n",
    "    mask = ~df_pubs[\"pmid\"].isin(pmids_text_adhoc_exist)\n",
    "    df_pubs = df_pubs[mask]\n",
    "    suffix = \"_\" + hashlib.sha256(\n",
    "        datetime.datetime.now().isoformat().encode(\"utf-8\")\n",
    "    ).hexdigest()[:8] # unique identifier\n",
    "\n",
    "# Load specter\n",
    "specter_adhoc = Specter(\"allenai/specter2_aug2023refresh\", adapter=\"adhoc_query\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd45418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176f1f0bd5054d1e9167d84c6a3f2ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1926 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode text in batches\n",
    "os.makedirs(data_dir / \"specter\", exist_ok=True)\n",
    "\n",
    "papers = [title + \"[SEP]\" + abstract\n",
    "          for title, abstract in zip(df_pubs['name'], df_pubs['description'])]\n",
    "\n",
    "for i in tqdm(range(0, len(papers), batch_size), total=len(papers)//batch_size):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        latent_specter_adhoc = specter_adhoc(papers[i:i+batch_size])\n",
    "\n",
    "    torch.save(\n",
    "        {\"embeddings\": latent_specter_adhoc, \"pmid\": df_pubs[\"pmid\"].values[i:i+batch_size]},\n",
    "        data_dir / \"specter\" / f\"encoded_text_specter2_adhoc_{str(i).zfill(5)}{suffix}.pt\",\n",
    "        pickle_protocol=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66eae71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack batched vectors\n",
    "latent_text_adhoc = torch.zeros((len(df_pubs), 768), dtype=torch.float32)\n",
    "pmids_text_adhoc = np.zeros(len(df_pubs), dtype=int)\n",
    "\n",
    "for idx in range(0, len(df_pubs), batch_size):\n",
    "    latent_text_adhoc[idx:idx+batch_size] , pmids_text_adhoc[idx:idx+batch_size] = torch.load(\n",
    "        data_dir / \"specter\" /  f\"encoded_text_specter2_adhoc_{str(idx).zfill(5)}{suffix}.pt\", weights_only=False\n",
    "    ).values()\n",
    "\n",
    "latent_text_adhoc = latent_text_adhoc / torch.norm(latent_text_adhoc, dim=1)[:, None]\n",
    "\n",
    "if suffix != \"\":\n",
    "    # Stack with existing\n",
    "    latent_text_adhoc = torch.vstack((latent_text_adhoc_exist, latent_text_adhoc))\n",
    "    pmids_text_adhoc = np.concatenate((pmids_text_adhoc_exist, pmids_text_adhoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a874c019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and save\n",
    "inds = np.argsort(pmids_text_adhoc)\n",
    "latent_text_adhoc = latent_text_adhoc[inds]\n",
    "pmids_text_adhoc = pmids_text_adhoc[inds]\n",
    "\n",
    "torch.save({\"latent\": latent_text_adhoc, \"pmid\": pmids_text_adhoc}, data_dir / \"latent_specter2_adhoc.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
