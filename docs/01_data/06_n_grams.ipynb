{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26337f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from neurovlm.retrieval_resources import (\n",
    "    _load_dataframe, _load_specter, _load_latent_text\n",
    ")\n",
    "from neurovlm.data import data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d567237d",
   "metadata": {},
   "source": [
    "# Corpus Extraction\n",
    "Extract n-grams for the training corpus. N-grams are weighted by cosine similarity to article embeddings, e.g. if n-gram is highly similar to the articles it gets a value near 1, otherwise it gets a value near 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54ec9b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(docs, ngram_range):\n",
    "    counts = CountVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        stop_words=\"english\",\n",
    "        min_df=1\n",
    "    ).fit(docs)\n",
    "\n",
    "    X = counts.transform(docs)  # shape: (n_docs, n_features)\n",
    "\n",
    "    feature_names = counts.get_feature_names_out()\n",
    "\n",
    "    mask = np.array(X.sum(axis=0) >= 100)[0]\n",
    "\n",
    "    X = np.array(X[:, mask].todense())\n",
    "    feature_names = feature_names[mask]\n",
    "\n",
    "    return X, feature_names\n",
    "\n",
    "# manual cleaning\n",
    "DROP_SUBSTRINGS = [\n",
    "    # study-like language\n",
    "    \"study\", \"studies\", \"result\", \"indicate\", \"show\", \"related\",\n",
    "    \"differences\", \"significant\", \"effect\", \"role\", \"measure\",\n",
    "    \"displayed\", \"involved\", \"examined\", \"associated\", \"altered\",\n",
    "    \"performed\", \"demonstrated\", \"conclus\", \"correlate\", \"individuals\",\n",
    "    \"common\", \"prior\",\n",
    "    # too general\n",
    "    \"brain\", \"neural\", \"neuroimaging\", \"mri\", \"fmri\", \"connectivity\",\n",
    "    \"diagnosed\", \"patients\", \"little\", \"known\", \"activation\", \"blood\",\n",
    "    \"alterations\", \"neuroscience\", \"people\",\n",
    "]\n",
    "\n",
    "DROP_REGEXES = [\n",
    "    r\"^cortex\",\n",
    "    # general single terms\n",
    "    r\"^ventral$\", r\"^frontal$\", r\"^neuronal$\", r\"^cognitive$\",\n",
    "    r\"^cerebral$\", r\"^resting_state$\",  r\"^disorder$\",\n",
    "    r\"^neuropsychological$\", r\"^cognition$\", r\"^stimulus$\",\n",
    "    r\"^dysfunction$\", r\"^imaging$\", r\"^functional$\",\n",
    "    r\"^functional imaging$\", r\"^task performance$\", r\"^impairments$\",\n",
    "    r\"^traits$\", r\"^dysfunction$\",  r\"^cognitive abilities$\", r\"^imaging dti$\",\n",
    "    # [SEP] token\n",
    "    r\"\\bsep\\b\",\n",
    "]\n",
    "\n",
    "pattern = \"|\".join(\n",
    "    [re.escape(s) for s in DROP_SUBSTRINGS] +  # plain terms\n",
    "    DROP_REGEXES                               # regex terms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f113ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_vocab_and_remap_X(\n",
    "    X: np.ndarray,\n",
    "    terms,\n",
    "    *,\n",
    "    force_keep=(),\n",
    "    drop_any_tokens=(\n",
    "        \"magnetic\",\"resonance\",\"imaging\",\"mri\",\"fmri\",\"functional\",\"scan\",\"scanner\",\"image\",\"data\",\n",
    "    ),\n",
    "    drop_first_tokens=(\n",
    "        \"use\",\"using\",\"undergo\",\"undergoing\",\"show\",\"showing\",\"change\",\"changes\",\n",
    "        \"investigate\",\"investigating\",\"examine\",\"examining\",\"perform\",\"performed\",\n",
    "    ),\n",
    "):\n",
    "    \"\"\"\n",
    "    Dense binary X (n_docs, n_terms) + term list -> cleaned vocab + remapped dense X via OR-merge.\n",
    "\n",
    "    Steps:\n",
    "      1) normalize\n",
    "      2) keep only \"nouny\" terms (heuristic) + force_keep\n",
    "      3) collapse contiguous subphrases into longest phrase\n",
    "      4) OR-merge columns into new X\n",
    "\n",
    "    Returns:\n",
    "      X_new: (n_docs, n_new) bool\n",
    "      new_terms: list[str]\n",
    "      old_to_new_term: dict[old_raw -> new_term or None]\n",
    "      old_to_new_idx: np.int32 (len=n_terms; -1 dropped)\n",
    "    \"\"\"\n",
    "    assert X.shape[1] == len(terms)\n",
    "\n",
    "    _acronym_re = re.compile(r\"^[a-z]{2,6}$\")\n",
    "    _vowel_re = re.compile(r\"[aeiou]\")\n",
    "\n",
    "    def norm(s: str) -> str:\n",
    "        s = s.lower().strip()\n",
    "        s = re.sub(r\"[^a-z0-9\\s\\-]\", \" \", s)\n",
    "        s = re.sub(r\"\\s+\", \" \", s)\n",
    "        return s\n",
    "\n",
    "    def tok(s: str):\n",
    "        return [w for w in norm(s).split() if w.isalpha()]\n",
    "\n",
    "    def is_acronym_like(s: str) -> bool:\n",
    "        s = norm(s).replace(\" \", \"\")\n",
    "        return s.isalpha() and (2 <= len(s) <= 6) and (_vowel_re.search(s) is None)  # dmn, pcc\n",
    "\n",
    "    def is_acronym(s: str) -> bool:\n",
    "        s = norm(s).replace(\" \", \"\")\n",
    "        return bool(_acronym_re.match(s))\n",
    "\n",
    "    drop_any_tokens = {norm(w) for w in drop_any_tokens}\n",
    "    drop_first_tokens = {norm(w) for w in drop_first_tokens}\n",
    "    force_keep = {norm(w) for w in force_keep}\n",
    "\n",
    "    terms = list(terms)\n",
    "    n_docs, n_terms = X.shape\n",
    "\n",
    "    # normalize + tokens\n",
    "    norm_terms = [norm(t) for t in terms]\n",
    "    toks = [tok(t) for t in norm_terms]\n",
    "\n",
    "    # noun keep mask (heuristic) + force_keep\n",
    "    keep = np.zeros(n_terms, dtype=bool)\n",
    "    canon_terms = [None] * n_terms\n",
    "\n",
    "    for j, (raw, nt, tt) in enumerate(zip(terms, norm_terms, toks)):\n",
    "        if nt in force_keep:\n",
    "            keep[j] = True\n",
    "            canon_terms[j] = nt\n",
    "            continue\n",
    "\n",
    "        if not tt:\n",
    "            continue\n",
    "\n",
    "        if len(tt) == 1:\n",
    "            # keep only acronym-ish unigrams (dmn/pcc) OR explicit force_keep handled above\n",
    "            if is_acronym_like(tt[0]) or is_acronym(tt[0]):\n",
    "                keep[j] = True\n",
    "                canon_terms[j] = tt[0]\n",
    "            continue\n",
    "\n",
    "        if tt[0] in drop_first_tokens:\n",
    "            continue\n",
    "        if any(w in drop_any_tokens for w in tt):\n",
    "            continue\n",
    "        if any(w.endswith((\"ing\", \"ed\")) for w in tt):\n",
    "            continue\n",
    "        if any(w in {\"of\",\"to\",\"in\",\"on\",\"with\",\"by\",\"for\",\"from\",\"as\",\"at\",\"and\",\"or\",\"the\",\"a\",\"an\"} for w in tt):\n",
    "            continue\n",
    "\n",
    "        keep[j] = True\n",
    "        canon_terms[j] = \" \".join(tt)\n",
    "\n",
    "    # contiguous-subphrase, collapse on token tuples\n",
    "    tok_tuples = [tuple(ct.split()) if ct else tuple() for ct in canon_terms]\n",
    "    keep_toks = {tok_tuples[j] for j in range(n_terms) if keep[j] and tok_tuples[j]}\n",
    "\n",
    "    supers = sorted(keep_toks, key=len, reverse=True)\n",
    "    sub_to_super = {}\n",
    "    dropped = set()\n",
    "\n",
    "    for super_tt in supers:\n",
    "        L = len(super_tt)\n",
    "        if L < 2:\n",
    "            continue\n",
    "        for i in range(L):\n",
    "            for k in range(i + 1, L + 1):\n",
    "                sub_tt = super_tt[i:k]\n",
    "                if len(sub_tt) < 2 or sub_tt == super_tt:\n",
    "                    continue\n",
    "                if sub_tt in keep_toks and sub_tt not in dropped:\n",
    "                    dropped.add(sub_tt)\n",
    "                    sub_to_super[sub_tt] = super_tt\n",
    "\n",
    "    final_toks = keep_toks - dropped\n",
    "    new_terms = sorted((\" \".join(tt) for tt in final_toks), key=lambda s: (len(s.split()), s))\n",
    "    new_index = {t: i for i, t in enumerate(new_terms)}\n",
    "\n",
    "    old_to_new_idx = np.full(n_terms, -1, dtype=np.int32)\n",
    "    old_to_new_term = {}\n",
    "\n",
    "    for j, raw in enumerate(terms):\n",
    "        if not keep[j] or not canon_terms[j]:\n",
    "            old_to_new_term[raw] = None\n",
    "            continue\n",
    "        tt = tok_tuples[j]\n",
    "        canon_tt = sub_to_super.get(tt, tt)\n",
    "        canon = \" \".join(canon_tt)\n",
    "        old_to_new_idx[j] = new_index[canon]\n",
    "        old_to_new_term[raw] = canon\n",
    "\n",
    "    # --- 4) OR-merge dense X\n",
    "    Xb = X.astype(bool, copy=False)\n",
    "    X_new = np.zeros((n_docs, len(new_terms)), dtype=bool)\n",
    "    for old_j, new_j in enumerate(old_to_new_idx):\n",
    "        if new_j >= 0:\n",
    "            X_new[:, new_j] |= Xb[:, old_j]\n",
    "\n",
    "    return X_new, new_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e0906e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load latent text\n",
    "latent, pmids = _load_latent_text()\n",
    "\n",
    "# load text\n",
    "df = _load_dataframe()\n",
    "text = df[\"name\"] + \" [SEP] \" + df[\"description\"]\n",
    "\n",
    "# extract\n",
    "X_uni, features_uni = extract_ngrams(text, (1, 1))\n",
    "X_bi, features_bi = extract_ngrams(text, (2, 2))\n",
    "X_tri, features_tri = extract_ngrams(text, (3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "800213c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((X_uni, X_bi, X_tri))\n",
    "features = np.concat((features_uni, features_bi, features_tri))\n",
    "\n",
    "# drop\n",
    "mask = ~pd.Series(features).str.contains(pattern, case=False, na=False, regex=True).to_numpy()\n",
    "features = features[mask]\n",
    "X = X[:, mask]\n",
    "X = X[df[\"pmid\"].argsort().to_numpy()]\n",
    "\n",
    "# clean\n",
    "X, features = clean_vocab_and_remap_X(X, features, force_keep=[\"precuneus\", \"working memory\", \"putamen\"])\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07a82205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually edited\n",
    "# corpus = pd.read_csv(\"corpus.txt\")\n",
    "# pd.concat((corpus, pd.DataFrame({\"terms\": features[mask], \"counts\": X.sum(axis=0)[mask]}))).sort_values(by=\"counts\", ascending=False).to_csv(\"corpus.txt\", index=False)\n",
    "# pd.DataFrame({\"terms\": features, \"counts\": X.sum(axis=0)}).sort_values(by=\"counts\", ascending=False).to_csv(\"corpus.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f264522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply manual filter\n",
    "corpus = pd.read_csv(\"corpus.txt\")\n",
    "mask = pd.Series(features).isin(corpus[\"terms\"])\n",
    "features = features[mask]\n",
    "X = X[:, mask]\n",
    "np.save(data_dir / \"ngram_matrix.npy\", X)\n",
    "np.save(data_dir / \"ngram_labels.npy\", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ec2261f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14db0d86fc9463cace8fb9c4644054d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# specter embeddings for ngrams\n",
    "specter = _load_specter()\n",
    "specter.specter = specter.specter.eval()\n",
    "\n",
    "# if not (data_dir / \"ngram_emb.pt\").exists():\n",
    "ngram_emb = []\n",
    "batch_size = 512\n",
    "for i in tqdm(range(0, len(features), batch_size), total=ceil(len(features)//batch_size)):\n",
    "    with torch.no_grad():\n",
    "        ngram_emb.append(specter(features[i:i+batch_size].tolist()))\n",
    "ngram_emb = torch.vstack(ngram_emb)\n",
    "ngram_emb = ngram_emb / ngram_emb.norm(dim=1)[:, None] # unit vector\n",
    "torch.save(ngram_emb, data_dir / \"ngram_emb.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
