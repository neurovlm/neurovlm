{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26337f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from neurovlm.retrieval_resources import (\n",
    "    _load_dataframe, _load_specter, _load_latent_text\n",
    ")\n",
    "from neurovlm.data import data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d567237d",
   "metadata": {},
   "source": [
    "# Corpus Extraction\n",
    "Extract n-grams for the training corpus. N-grams are weighted by cosine similarity to article embeddings, e.g. if n-gram is highly similar to the articles it gets a value near 1, otherwise it gets a value near 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54ec9b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(docs, ngram_range):\n",
    "    counts = CountVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        stop_words=\"english\",\n",
    "        min_df=1\n",
    "    ).fit(docs)\n",
    "\n",
    "    X = counts.transform(docs)  # shape: (n_docs, n_features)\n",
    "\n",
    "    feature_names = counts.get_feature_names_out()\n",
    "\n",
    "    mask = np.array(X.sum(axis=0) >= 100)[0]\n",
    "\n",
    "    X = np.array(X[:, mask].todense())\n",
    "    feature_names = feature_names[mask]\n",
    "\n",
    "    return X, feature_names\n",
    "\n",
    "# manual cleaning\n",
    "DROP_SUBSTRINGS = [\n",
    "    # study-like language\n",
    "    \"study\", \"studies\", \"result\", \"indicate\", \"show\", \"related\",\n",
    "    \"differences\", \"significant\", \"effect\", \"role\", \"measure\",\n",
    "    \"displayed\", \"involved\", \"examined\", \"associated\", \"altered\",\n",
    "    \"performed\", \"demonstrated\", \"conclus\", \"correlate\", \"individuals\",\n",
    "    \"common\", \"prior\",\n",
    "    # too general\n",
    "    \"brain\", \"neural\", \"neuroimaging\", \"mri\", \"fmri\", \"connectivity\",\n",
    "    \"diagnosed\", \"patients\", \"little\", \"known\", \"activation\", \"blood\",\n",
    "    \"alterations\", \"neuroscience\", \"people\",\n",
    "]\n",
    "\n",
    "DROP_REGEXES = [\n",
    "    r\"^cortex\",\n",
    "    # general single terms\n",
    "    r\"^ventral$\", r\"^frontal$\", r\"^neuronal$\", r\"^cognitive$\",\n",
    "    r\"^cerebral$\", r\"^resting_state$\",  r\"^disorder$\",\n",
    "    r\"^neuropsychological$\", r\"^cognition$\", r\"^stimulus$\",\n",
    "    r\"^dysfunction$\", r\"^imaging$\", r\"^functional$\",\n",
    "    r\"^functional imaging$\", r\"^task performance$\", r\"^impairments$\",\n",
    "    r\"^traits$\", r\"^dysfunction$\",  r\"^cognitive abilities$\", r\"^imaging dti$\",\n",
    "    # [SEP] token\n",
    "    r\"\\bsep\\b\",\n",
    "]\n",
    "\n",
    "pattern = \"|\".join(\n",
    "    [re.escape(s) for s in DROP_SUBSTRINGS] +  # plain terms\n",
    "    DROP_REGEXES                               # regex terms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a4519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "df = _load_dataframe()\n",
    "text = df[\"name\"] + \" [SEP] \" + df[\"description\"]\n",
    "\n",
    "# extract n-grams\n",
    "if not (data_dir / \"ngram_matrix.npy\").exists():\n",
    "    X_uni, features_uni = extract_ngrams(text, (1, 1))\n",
    "    X_bi, features_bi = extract_ngrams(text, (2, 2))\n",
    "    X_tri, features_tri = extract_ngrams(text, (3, 3))\n",
    "    X = np.hstack((X_uni, X_bi, X_tri))\n",
    "    features = np.concat((features_uni, features_bi, features_tri))\n",
    "    \n",
    "    mask = ~pd.Series(features).str.contains(pattern, case=False, na=False, regex=True).to_numpy()\n",
    "    features = features[mask]\n",
    "    X = X[:, mask]\n",
    "    X = X[df[\"pmid\"].argsort().to_numpy()]\n",
    "    np.save(data_dir / \"ngram_matrix.npy\", X)\n",
    "    np.save(data_dir / \"ngram_labels.npy\", features.astype(str))\n",
    "    np.save(data_dir / \"ngram_mask.npy\", mask)\n",
    "else:\n",
    "    # load pre-computed\n",
    "    X = np.load(data_dir / \"ngram_matrix.npy\")\n",
    "    features = np.load(data_dir / \"ngram_labels.npy\")\n",
    "\n",
    "# load latent text\n",
    "latent, pmids = _load_latent_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2261f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There are adapters available but none are activated for the forward pass.\n"
     ]
    }
   ],
   "source": [
    "# specter embeddings for ngrams\n",
    "specter = _load_specter()\n",
    "specter.specter = specter.specter.eval()\n",
    "\n",
    "if not (data_dir / \"ngram_emb.pt\").exists():\n",
    "    ngram_emb = []\n",
    "    batch_size = 512\n",
    "    for i in tqdm(range(0, len(features), batch_size), total=ceil(len(features)//batch_size)):\n",
    "        with torch.no_grad():\n",
    "            ngram_emb.append(specter(features[i:i+batch_size].tolist()))\n",
    "    ngram_emb = torch.vstack(ngram_emb)\n",
    "    ngram_emb = ngram_emb / ngram_emb.norm(dim=1)[:, None] # unit vector\n",
    "    torch.save(ngram_emb, data_dir / \"ngram_emb.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
