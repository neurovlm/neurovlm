{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation to validate the accuracy of the model trained on specter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to brian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import warnings\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "from neurovlm.data import fetch_data\n",
    "from neurovlm.models import NeuroAutoEncoder, TextAligner\n",
    "from neurovlm.train import Trainer, which_device\n",
    "\n",
    "device = torch.device('cpu')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load data once, onto the correct device\n",
    "latent_text = torch.load(\n",
    "    \"specter/latent_text.pt\", weights_only=False\n",
    ").to(\"cpu\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    latent_text_detach = torch.load(\"specter/latent_text.pt\").to(\"cpu\").detach()\n",
    "\n",
    "neuro_vectors = torch.load(\n",
    "    \"specter/neuro_vectors.pt\", weights_only=False\n",
    ").to(\"cpu\")\n",
    "\n",
    "neuro_vectors = neuro_vectors[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e40457070b48f39e084150a8968de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fold:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1/5 ---\n",
      "Validation BCE Loss: 0.6869108 (initial) -> 8.048 (current)))\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Validation BCE Loss: 0.6908949 (initial) -> 8.2855 (current)\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Validation BCE Loss: 0.6997182 (initial) -> 8.5162 (current))\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Validation BCE Loss: 0.6444784 (initial) -> 8.9977 (current))\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Validation BCE Loss: 0.6736145 (initial) -> 9.65 (current)t))\n",
      "\n",
      "Fold-wise losses: [0.6868982911109924, 0.6908074021339417, 0.6997487545013428, 0.6443724036216736, 0.6736149191856384]\n",
      "Mean validation loss over 5 folds: 0.6791\n"
     ]
    }
   ],
   "source": [
    "# Set up k-fold Cross-Validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Loss function for reconstruction\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "fold_val_losses = []\n",
    "\n",
    "# Outer loop with tqdm\n",
    "for fold_num, (train_idx, val_idx) in enumerate(\n",
    "        tqdm(kf.split(neuro_vectors), total=k, desc=\"Fold\"), start=1\n",
    "    ):\n",
    "    print(f\"\\n--- Fold {fold_num}/{k} ---\")\n",
    "\n",
    "    # Split `train_idx` further into sub-train and sub-test\n",
    "    train_sub_idx, test_sub_idx = train_test_split(\n",
    "        train_idx, train_size=0.8, random_state=0\n",
    "    )\n",
    "\n",
    "    # Build tensors for this fold\n",
    "    X_train = neuro_vectors[train_sub_idx]\n",
    "    X_test  = neuro_vectors[test_sub_idx]\n",
    "    X_val   = neuro_vectors[val_idx]\n",
    "\n",
    "    # 1) Train AutoEncoder\n",
    "    trainer = Trainer(\n",
    "        NeuroAutoEncoder(seed=fold_num),\n",
    "        n_epochs=100,\n",
    "        batch_size=256,\n",
    "        lr=1e-4,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        X_val=X_test,\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "\n",
    "    trainer.fit(X_train)\n",
    "    autoencoder = trainer.model\n",
    "\n",
    "    # 2) Train TextAligner\n",
    "    # Encode MNI vectors and fully detach to break any graph connectivity\n",
    "    with torch.no_grad():\n",
    "        latent_neuro = autoencoder.encoder(\n",
    "            neuro_vectors.to(\"cpu\")\n",
    "        )\n",
    "    latent_neuro = latent_neuro.detach().clone()\n",
    "\n",
    "    align_trainer = Trainer(\n",
    "        TextAligner(\n",
    "            latent_text_dim=latent_text.shape[-1],\n",
    "            hidden_dim=384,\n",
    "            latent_neuro_dim=latent_neuro.shape[-1],\n",
    "            seed=0\n",
    "        ),\n",
    "        n_epochs=500,\n",
    "        batch_size=1028,\n",
    "        lr=2e-4,\n",
    "        loss_fn=nn.MSELoss(),\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        X_val=latent_text_detach[test_sub_idx],\n",
    "        y_val=latent_neuro[test_sub_idx],\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "\n",
    "    align_trainer.fit(\n",
    "        latent_text_detach[train_sub_idx],\n",
    "        latent_neuro[train_sub_idx]\n",
    "    )\n",
    "\n",
    "    aligner = align_trainer.model\n",
    "\n",
    "    # 3) Compute final validation loss on the held-out fold\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_batch = latent_text[val_idx]\n",
    "        aligned_batch = aligner(latent_batch)\n",
    "        model_prediction = autoencoder.to('cpu').decoder(aligned_batch)\n",
    "        val_loss = loss_fn(model_prediction, X_val).item()\n",
    "\n",
    "    print(f\"Validation BCE Loss: {val_loss:.4f}\")\n",
    "    fold_val_losses.append(val_loss)\n",
    "\n",
    "# 4) Summarize across all folds\n",
    "mean_loss = sum(fold_val_losses) / k\n",
    "print(f\"\\nFold-wise losses: {fold_val_losses}\")\n",
    "print(f\"Mean validation loss over {k} folds: {mean_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tqdm.notebook import tqdm\n",
    "from neurovlm.models import NeuroAutoEncoder, TextAligner\n",
    "from neurovlm.train import Trainer, which_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine compute device\n",
    "# You can set this to \"cpu\" or use which_device() to auto-select\n",
    "device = \"cpu\"\n",
    "\n",
    "# Load precomputed latents and brain vectors\n",
    "latent_text = torch.load(\n",
    "    \"specter/latent_text.pt\",\n",
    "    map_location=device\n",
    ").to(device)\n",
    "# Detached copy for aligner training\n",
    "latent_text_detach = latent_text.detach().clone()\n",
    "\n",
    "neuro_vectors = torch.load(\n",
    "    \"specter/neuro_vectors.pt\",\n",
    "    map_location=device\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb70f43baabf43b4851fd7d42a96784f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fold:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1/5 ---\n",
      "Fold 1 avg top-5 cos sim: 0.9456initial) -> 8.048 (current)))\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Fold 2 avg top-5 cos sim: 0.9403initial) -> 8.2855 (current)\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Fold 3 avg top-5 cos sim: 0.9456initial) -> 8.5162 (current))\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Fold 4 avg top-5 cos sim: 0.9456initial) -> 8.9977 (current))\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Fold 5 avg top-5 cos sim: 0.9432initial) -> 9.65 (current)t))\n",
      "\n",
      "Fold-wise scores: [np.float32(0.94562376), np.float32(0.9403087), np.float32(0.94555545), np.float32(0.9455957), np.float32(0.9432132)]\n",
      "Mean avg top-5 cos sim over 5 folds: 0.9441\n"
     ]
    }
   ],
   "source": [
    "# 5-fold CV\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "fold_scores = []\n",
    "top_k = 5\n",
    "\n",
    "for fold_num, (train_idx, val_idx) in enumerate(\n",
    "        tqdm(kf.split(neuro_vectors), total=k, desc=\"Fold\"), start=1\n",
    "    ):\n",
    "    print(f\"\\n--- Fold {fold_num}/{k} ---\")\n",
    "\n",
    "    # Split train into sub-train, sub-test\n",
    "    train_sub_idx, test_sub_idx = train_test_split(\n",
    "        train_idx, train_size=0.8, random_state=0\n",
    "    )\n",
    "\n",
    "    # Subsets\n",
    "    X_train = neuro_vectors[train_sub_idx]\n",
    "    X_test  = neuro_vectors[test_sub_idx]\n",
    "\n",
    "    # 1) Train AutoEncoder on sub-train\n",
    "    ae_trainer = Trainer(\n",
    "        NeuroAutoEncoder(seed=fold_num),\n",
    "        n_epochs=100,\n",
    "        batch_size=256,\n",
    "        lr=1e-4,\n",
    "        loss_fn=nn.BCELoss(),\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        X_val=X_test,\n",
    "        device=device\n",
    "    )\n",
    "    ae_trainer.fit(X_train)\n",
    "    autoencoder = ae_trainer.model.eval()\n",
    "\n",
    "    # 2) Train TextAligner\n",
    "    # Encode all neuro into detached tensor\n",
    "    with torch.no_grad():\n",
    "        latent_neuro = autoencoder.encoder(neuro_vectors)\n",
    "    latent_neuro = latent_neuro.detach().clone()\n",
    "\n",
    "    align_trainer = Trainer(\n",
    "        TextAligner(\n",
    "            latent_text_dim=latent_text.shape[-1],\n",
    "            hidden_dim=384,\n",
    "            latent_neuro_dim=latent_neuro.shape[-1],\n",
    "            seed=0\n",
    "        ),\n",
    "        n_epochs=500,\n",
    "        batch_size=1024,\n",
    "        lr=2e-4,\n",
    "        loss_fn=nn.MSELoss(),\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        X_val=latent_text_detach[test_sub_idx],\n",
    "        y_val=latent_neuro[test_sub_idx],\n",
    "        device=device\n",
    "    )\n",
    "    align_trainer.fit(\n",
    "        latent_text_detach[train_sub_idx],\n",
    "        latent_neuro[train_sub_idx]\n",
    "    )\n",
    "    aligner = align_trainer.model.eval()\n",
    "\n",
    "    # 3) Evaluate retrieval on held-out fold\n",
    "    # Candidate documents: those in the outer training set\n",
    "    docs = latent_text[train_idx]\n",
    "    with torch.no_grad():\n",
    "        aligned_docs = aligner(docs)\n",
    "    docs_np = aligned_docs.detach().cpu().numpy().astype(np.float32)\n",
    "    docs_norm = docs_np / np.linalg.norm(docs_np, axis=1, keepdims=True)\n",
    "\n",
    "    # Query brain vectors for validation set\n",
    "    with torch.no_grad():\n",
    "        q_neuro = autoencoder.encoder(neuro_vectors[val_idx])\n",
    "    queries_np = q_neuro.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    # Compute average top-k cosine similarity per query\n",
    "    fold_similarities = []\n",
    "    for vec in queries_np:\n",
    "        vec_norm = vec / np.linalg.norm(vec)\n",
    "        sims = docs_norm @ vec_norm\n",
    "        topk = np.sort(sims)[-top_k:]\n",
    "        fold_similarities.append(topk.mean())\n",
    "\n",
    "    fold_score = np.mean(fold_similarities)\n",
    "    print(f\"Fold {fold_num} avg top-{top_k} cos sim: {fold_score:.4f}\")\n",
    "    fold_scores.append(fold_score)\n",
    "\n",
    "# Summary\n",
    "mean_score = np.mean(fold_scores)\n",
    "print(f\"\\nFold-wise scores: {fold_scores}\")\n",
    "print(f\"Mean avg top-{top_k} cos sim over {k} folds: {mean_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask ryan is the loss is\n",
    "# 1. average cosine similarity between the between the unseen brain and the top k most similar title&abstract pairs\n",
    "# 2. or the average cosine similarity between the unseen brain's title & abstract vector and the top k most similar title&abstract pairs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
