{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4863507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from neurovlm.data import get_data_dir\n",
    "from neurovlm.train import Trainer, which_device\n",
    "from neurovlm.models import TextAligner\n",
    "from neurovlm.loss import InfoNCELoss\n",
    "\n",
    "device = which_device()\n",
    "data_dir = get_data_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b2c129",
   "metadata": {},
   "source": [
    "# Projection Head\n",
    "\n",
    "Projection head refers to a small network to align the latent spaces between text and neuroimages. The training regime starts with MSELoss, then gradually removed the influences of outliers through truncation, i.e. masking out the top-k% of loss instances from gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b3d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoded neurovectors from the second notebook\n",
    "latent_neuro, pmids_latent = torch.load(data_dir / \"latent_neuro_sparse.pt\", weights_only=False).values()\n",
    "\n",
    "# Load encoded text from last notebook\n",
    "latent_text_specter, pmids = torch.load(data_dir / \"latent_specter2_adhoc.pt\", weights_only=False).values()\n",
    "inds = np.argsort(pmids)\n",
    "latent_text_specter, pmids = latent_text_specter[inds], pmids[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01f32e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse\n",
    "mask =  pd.Series(pmids).isin(pmids_latent)\n",
    "pmids = pmids[mask]\n",
    "latent_text_specter = latent_text_specter[mask]\n",
    "assert (pmids == pmids_latent).all()\n",
    "\n",
    "mask = latent_neuro.norm(dim=1).detach().cpu().numpy() < 35 # sparser targets\n",
    "latent_neuro = latent_neuro[mask]\n",
    "latent_text_specter = latent_text_specter[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae29002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dict(\n",
    "    latent=latent_neuro,\n",
    "    pmid=pmids[mask],\n",
    "), data_dir/\"latent_neuro_sparse.pt\")\n",
    "\n",
    "torch.save(dict(\n",
    "    latent=latent_text_specter,\n",
    "    pmid=pmids[mask],\n",
    "), data_dir/\"latent_text_sparse.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13c0023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test/validation split\n",
    "inds = torch.arange(len(latent_neuro))\n",
    "train_inds, test_inds = train_test_split(\n",
    "    inds, train_size=0.8, random_state=0\n",
    ")\n",
    "test_inds, val_inds = train_test_split(\n",
    "    test_inds, train_size=0.5, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc288dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626b883815da4156b7f847028c652db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, val loss: 7.8888\n",
      "Epoch: 1, val loss: 7.8262\n",
      "Epoch: 2, val loss: 7.7599\n",
      "Epoch: 3, val loss: 7.7002\n",
      "Epoch: 4, val loss: 7.6519\n",
      "Epoch: 5, val loss: 7.6101\n",
      "Epoch: 6, val loss: 7.5721\n",
      "Epoch: 7, val loss: 7.536\n",
      "Epoch: 8, val loss: 7.5012\n",
      "Epoch: 9, val loss: 7.4705\n",
      "Epoch: 10, val loss: 7.4407\n",
      "Epoch: 11, val loss: 7.4154\n",
      "Epoch: 12, val loss: 7.3923\n",
      "Epoch: 13, val loss: 7.3734\n",
      "Epoch: 14, val loss: 7.3578\n",
      "Epoch: 15, val loss: 7.343\n",
      "Epoch: 16, val loss: 7.3279\n",
      "Epoch: 17, val loss: 7.3169\n",
      "Epoch: 18, val loss: 7.3055\n",
      "Epoch: 19, val loss: 7.2949\n",
      "Epoch: 20, val loss: 7.2855\n",
      "Epoch: 21, val loss: 7.2787\n",
      "Epoch: 22, val loss: 7.273\n",
      "Epoch: 23, val loss: 7.2653\n",
      "Epoch: 24, val loss: 7.258\n",
      "Epoch: 25, val loss: 7.2542\n",
      "Epoch: 26, val loss: 7.2478\n",
      "Epoch: 27, val loss: 7.2431\n",
      "Epoch: 28, val loss: 7.2385\n",
      "Epoch: 29, val loss: 7.2364\n",
      "Epoch: 30, val loss: 7.2302\n",
      "Epoch: 31, val loss: 7.2252\n",
      "Epoch: 32, val loss: 7.2208\n",
      "Epoch: 33, val loss: 7.2159\n",
      "Epoch: 34, val loss: 7.2129\n",
      "Epoch: 35, val loss: 7.2111\n",
      "Epoch: 36, val loss: 7.2058\n",
      "Epoch: 37, val loss: 7.2031\n",
      "Epoch: 38, val loss: 7.2018\n",
      "Epoch: 39, val loss: 7.1967\n",
      "Epoch: 40, val loss: 7.1966\n",
      "Epoch: 41, val loss: 7.1923\n",
      "Epoch: 42, val loss: 7.1914\n",
      "Epoch: 43, val loss: 7.1868\n",
      "Epoch: 44, val loss: 7.1861\n",
      "Epoch: 45, val loss: 7.1842\n",
      "Epoch: 46, val loss: 7.1807\n",
      "Epoch: 47, val loss: 7.1793\n",
      "Epoch: 48, val loss: 7.1784\n",
      "Epoch: 49, val loss: 7.1771\n",
      "Epoch: 50, val loss: 7.1738\n",
      "Epoch: 51, val loss: 7.172\n",
      "Epoch: 52, val loss: 7.1707\n",
      "Epoch: 53, val loss: 7.1692\n",
      "Epoch: 54, val loss: 7.1693\n",
      "Epoch: 55, val loss: 7.1664\n",
      "Epoch: 56, val loss: 7.1651\n",
      "Epoch: 57, val loss: 7.1629\n",
      "Epoch: 58, val loss: 7.1606\n",
      "Epoch: 59, val loss: 7.1608\n",
      "Epoch: 60, val loss: 7.1604\n",
      "Epoch: 61, val loss: 7.1574\n",
      "Epoch: 62, val loss: 7.1566\n",
      "Epoch: 63, val loss: 7.1568\n",
      "Epoch: 64, val loss: 7.1539\n",
      "Epoch: 65, val loss: 7.1537\n",
      "Epoch: 66, val loss: 7.1509\n",
      "Epoch: 67, val loss: 7.1493\n",
      "Epoch: 68, val loss: 7.1501\n",
      "Epoch: 69, val loss: 7.1471\n",
      "Epoch: 70, val loss: 7.1467\n",
      "Epoch: 71, val loss: 7.1462\n",
      "Epoch: 72, val loss: 7.1472\n",
      "Epoch: 73, val loss: 7.1465\n",
      "Epoch: 74, val loss: 7.1432\n",
      "Epoch: 75, val loss: 7.1424\n",
      "Epoch: 76, val loss: 7.1416\n",
      "Epoch: 77, val loss: 7.1431\n",
      "Epoch: 78, val loss: 7.14\n",
      "Epoch: 79, val loss: 7.139\n",
      "Epoch: 80, val loss: 7.1415\n",
      "Epoch: 81, val loss: 7.1388\n",
      "Epoch: 82, val loss: 7.1367\n",
      "Epoch: 83, val loss: 7.1378\n",
      "Epoch: 84, val loss: 7.1362\n",
      "Epoch: 85, val loss: 7.1343\n",
      "Epoch: 86, val loss: 7.1338\n",
      "Epoch: 87, val loss: 7.1329\n",
      "Epoch: 88, val loss: 7.1329\n",
      "Epoch: 89, val loss: 7.1315\n",
      "Epoch: 90, val loss: 7.1324\n",
      "Epoch: 91, val loss: 7.1307\n",
      "Epoch: 92, val loss: 7.1297\n",
      "Epoch: 93, val loss: 7.1319\n",
      "Epoch: 94, val loss: 7.1305\n",
      "Epoch: 95, val loss: 7.1313\n",
      "Epoch: 96, val loss: 7.1287\n",
      "Epoch: 97, val loss: 7.1282\n",
      "Epoch: 98, val loss: 7.1311\n",
      "Epoch: 99, val loss: 7.1269\n",
      "Epoch: 100, val loss: 7.1266\n",
      "Epoch: 101, val loss: 7.1274\n",
      "Epoch: 102, val loss: 7.1284\n",
      "Epoch: 103, val loss: 7.1265\n",
      "Epoch: 104, val loss: 7.1273\n",
      "Epoch: 105, val loss: 7.1257\n",
      "Epoch: 106, val loss: 7.127\n",
      "Epoch: 107, val loss: 7.1279\n",
      "Epoch: 108, val loss: 7.1244\n",
      "Epoch: 109, val loss: 7.1282\n",
      "Epoch: 110, val loss: 7.1233\n",
      "Epoch: 111, val loss: 7.125\n",
      "Epoch: 112, val loss: 7.1239\n",
      "Epoch: 113, val loss: 7.121\n",
      "Epoch: 114, val loss: 7.1274\n",
      "Epoch: 115, val loss: 7.1239\n",
      "Epoch: 116, val loss: 7.1241\n",
      "Epoch: 117, val loss: 7.1235\n",
      "Epoch: 118, val loss: 7.1228\n",
      "Epoch: 119, val loss: 7.1258\n",
      "Epoch: 120, val loss: 7.1251\n",
      "Epoch: 121, val loss: 7.1236\n",
      "Epoch: 122, val loss: 7.1249\n",
      "Epoch: 123, val loss: 7.1225\n",
      "Epoch: 124, val loss: 7.1228\n",
      "Epoch: 125, val loss: 7.1221\n",
      "Epoch: 126, val loss: 7.1233\n",
      "Epoch: 127, val loss: 7.1227\n",
      "Epoch: 128, val loss: 7.1236\n",
      "Epoch: 129, val loss: 7.1249\n",
      "Epoch: 130, val loss: 7.1241\n",
      "Epoch: 131, val loss: 7.1259\n",
      "Epoch: 132, val loss: 7.1212\n",
      "Epoch: 133, val loss: 7.1228\n",
      "Epoch: 134, val loss: 7.1225\n",
      "Epoch: 135, val loss: 7.1246\n",
      "Epoch: 136, val loss: 7.1214\n",
      "Epoch: 137, val loss: 7.1232\n",
      "Epoch: 138, val loss: 7.1252\n",
      "Epoch: 139, val loss: 7.1237\n",
      "Epoch: 140, val loss: 7.1303\n",
      "Epoch: 141, val loss: 7.1238\n",
      "Epoch: 142, val loss: 7.1218\n",
      "Epoch: 143, val loss: 7.1211\n",
      "Epoch: 144, val loss: 7.1234\n",
      "Epoch: 145, val loss: 7.1249\n",
      "Epoch: 146, val loss: 7.1252\n",
      "Epoch: 147, val loss: 7.1233\n",
      "Epoch: 148, val loss: 7.1246\n",
      "Epoch: 149, val loss: 7.1254\n",
      "Epoch: 150, val loss: 7.123\n",
      "Epoch: 151, val loss: 7.1235\n",
      "Epoch: 152, val loss: 7.1233\n",
      "Epoch: 153, val loss: 7.1214\n",
      "Epoch: 154, val loss: 7.1252\n",
      "Epoch: 155, val loss: 7.1249\n",
      "Epoch: 156, val loss: 7.1229\n",
      "Epoch: 157, val loss: 7.1256\n",
      "Epoch: 158, val loss: 7.1248\n",
      "Epoch: 159, val loss: 7.1252\n",
      "Epoch: 160, val loss: 7.125\n",
      "Epoch: 161, val loss: 7.125\n",
      "Epoch: 162, val loss: 7.1269\n",
      "Epoch: 163, val loss: 7.1251\n",
      "Epoch: 164, val loss: 7.1266\n",
      "Epoch: 165, val loss: 7.1256\n",
      "Epoch: 166, val loss: 7.1276\n",
      "Epoch: 167, val loss: 7.127\n",
      "Epoch: 168, val loss: 7.1277\n",
      "Epoch: 169, val loss: 7.128\n",
      "Epoch: 170, val loss: 7.1277\n",
      "Epoch: 171, val loss: 7.1272\n",
      "Epoch: 172, val loss: 7.1292\n",
      "Epoch: 173, val loss: 7.1278\n",
      "Epoch: 174, val loss: 7.1284\n",
      "Epoch: 175, val loss: 7.1302\n",
      "Epoch: 176, val loss: 7.1271\n",
      "Epoch: 177, val loss: 7.132\n",
      "Epoch: 178, val loss: 7.1292\n",
      "Epoch: 179, val loss: 7.127\n",
      "Epoch: 180, val loss: 7.1337\n",
      "Epoch: 181, val loss: 7.1328\n",
      "Epoch: 182, val loss: 7.1279\n",
      "Epoch: 183, val loss: 7.1313\n",
      "Epoch: 184, val loss: 7.1322\n",
      "Epoch: 185, val loss: 7.1294\n",
      "Epoch: 186, val loss: 7.1312\n",
      "Epoch: 187, val loss: 7.1343\n",
      "Epoch: 188, val loss: 7.1331\n",
      "Epoch: 189, val loss: 7.1347\n",
      "Epoch: 190, val loss: 7.1377\n",
      "Epoch: 191, val loss: 7.1343\n",
      "Epoch: 192, val loss: 7.1318\n",
      "Epoch: 193, val loss: 7.1348\n",
      "Epoch: 194, val loss: 7.1356\n",
      "Epoch: 195, val loss: 7.1374\n",
      "Epoch: 196, val loss: 7.1372\n",
      "Epoch: 197, val loss: 7.1351\n",
      "Epoch: 198, val loss: 7.1358\n",
      "Epoch: 199, val loss: 7.1369\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim import AdamW\n",
    "from neurovlm.loss import InfoNCELoss\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set device\n",
    "device = \"mps\"\n",
    "\n",
    "X_train_image = latent_neuro[train_inds].to(device)\n",
    "X_train_text  = latent_text_specter[train_inds].to(device)\n",
    "X_val_image = latent_neuro[val_inds].to(device)\n",
    "X_val_text  = latent_text_specter[val_inds].to(device)\n",
    "\n",
    "# Models\n",
    "proj_head_text  = TextAligner(seed=123, latent_text_dim=768, hidden_dim=512, latent_neuro_dim=384).to(device)\n",
    "proj_head_image = TextAligner(seed=123, latent_text_dim=384, hidden_dim=384, latent_neuro_dim=384).to(device)\n",
    "\n",
    "# Settings\n",
    "loss_fn = InfoNCELoss()\n",
    "n_epochs = 200\n",
    "batch_size = 512\n",
    "lr = 1e-5\n",
    "optimizer = AdamW([*proj_head_text.parameters(), *proj_head_image.parameters()], lr=lr, weight_decay=1e-4)  # small wd helps\n",
    "interval = 1\n",
    "max_grad_norm = 1.0  # gradient clipping\n",
    "\n",
    "# Train\n",
    "iterable = tqdm(range(n_epochs), total=n_epochs)\n",
    "\n",
    "for iepoch in iterable:\n",
    "    proj_head_text.train()\n",
    "    proj_head_image.train()\n",
    "\n",
    "    # Randomly shuffle data\n",
    "    torch.manual_seed(iepoch)\n",
    "    rand_inds = torch.randperm(len(X_train_image), device=device)\n",
    "\n",
    "    for i in range(0, len(X_train_image), batch_size):\n",
    "        idx = rand_inds[i:i+batch_size]\n",
    "\n",
    "        # Forward\n",
    "        y_text  = proj_head_text(X_train_text[idx])\n",
    "        y_image = proj_head_image(X_train_image[idx])\n",
    "\n",
    "        # Loss\n",
    "        loss = loss_fn(y_text, y_image)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip to tame spikes\n",
    "        # clip_grad_norm_([*proj_head_text.parameters(), *proj_head_image.parameters()], max_grad_norm)\n",
    "\n",
    "        # Step\n",
    "        optimizer.step()\n",
    "\n",
    "    if iepoch % interval == 0:\n",
    "        proj_head_text.eval()\n",
    "        proj_head_image.eval()\n",
    "        with torch.no_grad():\n",
    "            y_text  = proj_head_text(X_val_text)\n",
    "            y_image = proj_head_image(X_val_image)\n",
    "\n",
    "            y_text  = F.normalize(y_text,  dim=-1)\n",
    "            y_image = F.normalize(y_image, dim=-1)\n",
    "\n",
    "            val_loss = loss_fn(y_text, y_image)\n",
    "            print(f\"Epoch: {iepoch}, val loss: {float(val_loss):.5g}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "889582be",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(proj_head_text, data_dir / \"proj_head_text_infonce.pt\")\n",
    "torch.save(proj_head_image, data_dir / \"proj_head_image_infonce.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
