{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec78d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from safetensors.torch import save_file\n",
    "import warnings\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "\n",
    "\n",
    "import nibabel as nib\n",
    "from nilearn.datasets import fetch_atlas_harvard_oxford,load_mni152_template\n",
    "from nilearn.plotting import plot_glass_brain, view_img\n",
    "from nilearn.image import load_img, smooth_img, resample_img, coord_transform,resample_to_img\n",
    "from nilearn import datasets\n",
    "from nilearn.maskers import NiftiMasker\n",
    "\n",
    "from neurovlm.data import fetch_data\n",
    "from neurovlm.coords import coords_to_vectors\n",
    "from neurovlm.models import NeuroAutoEncoder, TextAligner\n",
    "from neurovlm.train import Trainer, which_device\n",
    "device = which_device()\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "from neuroquery import datasets\n",
    "from neuroquery.img_utils import gaussian_coord_smoothing, coords_to_peaks_img\n",
    "from neuroquery._compat import maskers, load_mni152_brain_mask\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "from neurovlm.data import fetch_data\n",
    "from neurovlm.models import NeuroAutoEncoder, TextAligner\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d677ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = fetch_data()\n",
    "df_pubs = pd.read_parquet(f\"{data_dir}/publications.parquet\")\n",
    "df_coords = pd.read_parquet(f\"{data_dir}/coordinates.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "610672f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained\n",
    "neuro_encoder = torch.load(\n",
    "    \"docs/autoencoder.pt\", weights_only=False\n",
    ").to(\"cpu\")\n",
    "\n",
    "neuro_decoder = torch.load(\n",
    "    \"docs/decoder_half.pt\", weights_only=False\n",
    ").to(\"cpu\")\n",
    "\n",
    "text_aligner_half = torch.load(\n",
    "    \"specter/aligner_half.pt\", weights_only=False\n",
    ").to(\"cpu\")\n",
    "\n",
    "text_aligner = torch.load(\n",
    "    \"specter/aligner.pt\", weights_only=False\n",
    ").to(\"cpu\")\n",
    "\n",
    "\n",
    "latent_titles = torch.load(\n",
    "    \"specter/latent_text.pt\", weights_only=False\n",
    ").to(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "307bcebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28757, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_titles.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aec8716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28393"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pubs.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a7530ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from latent_titles\n",
    "unique_titles = torch.unique(latent_titles, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e87620cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28472, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_titles.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d89ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total number of batches\n",
    "total_batches = 3550\n",
    "chunk_size = total_batches // 4\n",
    "\n",
    "def process_quarter(start_idx, end_idx, quarter_name):\n",
    "    print(f\"Processing {quarter_name}...\")\n",
    "    latent_text_batches = []\n",
    "    for i in tqdm(range(start_idx, end_idx + 1)):\n",
    "        batch_tensor = torch.load(f\"batches/latent_text_batch_{i}.pt\")\n",
    "        latent_text_batches.append(batch_tensor)\n",
    "    \n",
    "    # Concatenate batches\n",
    "    quarter_tensor = torch.cat(latent_text_batches, dim=0)\n",
    "    \n",
    "    # Remove duplicates within the quarter\n",
    "    unique_tensor, inverse_indices = torch.unique(quarter_tensor, dim=0, return_inverse=True)\n",
    "    print(f\"{quarter_name} unique vectors: {len(unique_tensor)} (removed {len(quarter_tensor) - len(unique_tensor)} duplicates)\")\n",
    "    \n",
    "    torch.save(unique_tensor, f\"latent_text_{quarter_name}.pt\")\n",
    "    return unique_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185d5a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing first_quarter...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9680b8a6fb1f418db442911d00716712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/887 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process each quarter\n",
    "latent_text_first = process_quarter(1, chunk_size, \"first_quarter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ad7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_text_second = process_quarter(chunk_size + 1, chunk_size * 2, \"second_quarter\")\n",
    "latent_text_third = process_quarter(chunk_size * 2 + 1, chunk_size * 3, \"third_quarter\")\n",
    "latent_text_fourth = process_quarter(chunk_size * 3 + 1, total_batches, \"fourth_quarter\")\n",
    "\n",
    "# Combine all quarters and remove any remaining duplicates\n",
    "print(\"Combining quarters and removing final duplicates...\")\n",
    "latent_text = torch.cat([\n",
    "    latent_text_first,\n",
    "    latent_text_second,\n",
    "    latent_text_third,\n",
    "    latent_text_fourth\n",
    "], dim=0)\n",
    "\n",
    "# Remove any duplicates that might exist between quarters\n",
    "unique_latent_text, inverse_indices = torch.unique(latent_text, dim=0, return_inverse=True)\n",
    "print(f\"Final unique vectors: {len(unique_latent_text)} (removed {len(latent_text) - len(unique_latent_text)} duplicates)\")\n",
    "torch.save(unique_latent_text, \"latent_text.pt\")\n",
    "\n",
    "print(\"Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557b100",
   "metadata": {},
   "source": [
    "# Map to brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aad39424",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df_pubs['name'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb4ef305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Align latent text using aligner_half\n",
    "# aligned_titles_half_list = []\n",
    "# for title_vec in latent_titles:\n",
    "#     aligned_vec = text_aligner_half(title_vec.unsqueeze(0))\n",
    "#     aligned_titles_half_list.append(aligned_vec)\n",
    "# aligned_titles_half = torch.cat(aligned_titles_half_list, dim=0)\n",
    "# torch.save(aligned_titles_half, \"specter/aligned_titles_half.pt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "060811df",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_text = torch.load(\n",
    "    \"specter/aligned_text.pt\", weights_only=False\n",
    ").to(\"cpu\")\n",
    "# aligned_titles_half\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5473db3",
   "metadata": {},
   "source": [
    "# Cosine similarity between user input and aligned tensor\n",
    "\n",
    "Compute query vector, take the top-k most related studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aedaf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fetch_neurovault] fetch_neurovault: using default value of 100 for max_images. Set max_images to another value or None if you want more images.\n",
      "[get_dataset_dir] Dataset found in /Users/borng/nilearn_data/neurovault\n",
      "[fetch_neurovault] Reading local neurovault data.\n",
      "[fetch_neurovault] Already fetched 1 image\n",
      "[fetch_neurovault] Already fetched 2 images\n",
      "[fetch_neurovault] Already fetched 3 images\n",
      "[fetch_neurovault] Already fetched 4 images\n",
      "[fetch_neurovault] Already fetched 5 images\n",
      "[fetch_neurovault] Already fetched 6 images\n",
      "[fetch_neurovault] Already fetched 7 images\n",
      "[fetch_neurovault] Already fetched 8 images\n",
      "[fetch_neurovault] Already fetched 9 images\n",
      "[fetch_neurovault] Already fetched 10 images\n",
      "[fetch_neurovault] Already fetched 11 images\n",
      "[fetch_neurovault] Already fetched 12 images\n",
      "[fetch_neurovault] Already fetched 13 images\n",
      "[fetch_neurovault] Already fetched 14 images\n",
      "[fetch_neurovault] Already fetched 15 images\n",
      "[fetch_neurovault] Already fetched 16 images\n",
      "[fetch_neurovault] Already fetched 17 images\n",
      "[fetch_neurovault] Already fetched 18 images\n",
      "[fetch_neurovault] Already fetched 19 images\n",
      "[fetch_neurovault] Already fetched 20 images\n",
      "[fetch_neurovault] Already fetched 21 images\n",
      "[fetch_neurovault] Already fetched 22 images\n",
      "[fetch_neurovault] Already fetched 23 images\n",
      "[fetch_neurovault] Already fetched 24 images\n",
      "[fetch_neurovault] Already fetched 25 images\n",
      "[fetch_neurovault] Already fetched 26 images\n",
      "[fetch_neurovault] Already fetched 27 images\n",
      "[fetch_neurovault] Already fetched 28 images\n",
      "[fetch_neurovault] Already fetched 29 images\n",
      "[fetch_neurovault] Already fetched 30 images\n",
      "[fetch_neurovault] Already fetched 31 images\n",
      "[fetch_neurovault] Already fetched 32 images\n",
      "[fetch_neurovault] Already fetched 33 images\n",
      "[fetch_neurovault] Already fetched 34 images\n",
      "[fetch_neurovault] Already fetched 35 images\n",
      "[fetch_neurovault] Already fetched 36 images\n",
      "[fetch_neurovault] Already fetched 37 images\n",
      "[fetch_neurovault] Already fetched 38 images\n",
      "[fetch_neurovault] Already fetched 39 images\n",
      "[fetch_neurovault] Already fetched 40 images\n",
      "[fetch_neurovault] Already fetched 41 images\n",
      "[fetch_neurovault] 41 images found on local disk.\n"
     ]
    }
   ],
   "source": [
    "from nilearn.datasets import fetch_neurovault\n",
    "maps = fetch_neurovault(collection_ids=[1039], get_data=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8295228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfrom_nifti_to_2d(nifti_img, mask_img_path=None):\n",
    "    \"\"\"\n",
    "    Transforms a NIfTI image into a 2D flattened array using a mask image.\n",
    "    \n",
    "    Parameters:\n",
    "    - nifti_img: nibabel NIfTI image object.\n",
    "    - mask_img_path: str, optional path to a mask image. If not provided, \n",
    "      the function uses the NeuroQuery model mask.\n",
    "      \n",
    "    Returns:\n",
    "    - out: np.ndarray, the 2D flattened array representation of the NIfTI image.\n",
    "    \"\"\"\n",
    "    region_img = (nifti_img.get_fdata()).astype(float)\n",
    "\n",
    "    region_nii = nib.Nifti1Image(region_img.astype(np.int32), nifti_img.affine, dtype=np.int32)\n",
    "\n",
    "    # Load the mask image; if no path is provided, load from the NeuroQuery model.\n",
    "    if mask_img_path is None:\n",
    "        mask_img = load_img(f\"{datasets.fetch_neuroquery_model()}/mask_img.nii.gz\", dtype=np.float32)\n",
    "    else:\n",
    "        mask_img = load_img(mask_img_path, dtype=np.float32)\n",
    "\n",
    "    masker = NiftiMasker(mask_img=mask_img, dtype=np.float32).fit()\n",
    "\n",
    "    region_nii_resampled = resample_to_img(\n",
    "        region_nii, mask_img, interpolation='nearest', force_resample=True, copy_header=False\n",
    "    )\n",
    "    \n",
    "    # Transform the resampled region image into a 2D array (flattened) using the masker.\n",
    "    out = masker.transform(region_nii_resampled)\n",
    "\n",
    "    out[out > 0] = 1\n",
    "\n",
    "    out_tensor = torch.tensor(out).squeeze(0)\n",
    "\n",
    "    return out_tensor\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c648b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2d3e80030041238a98ea08ee3fe2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total NIfTI files loaded: 41\n"
     ]
    }
   ],
   "source": [
    "# Path to the NeuroVault collection folder\n",
    "path = \"/Users/borng/nilearn_data/neurovault/collection_1039\"\n",
    "\n",
    "# Make sure the path exists\n",
    "if not os.path.exists(path):\n",
    "    raise FileNotFoundError(f\"Path does not exist: {path}\")\n",
    "\n",
    "# Loop through and load all .nii.gz files\n",
    "nifti_files = [f for f in os.listdir(path) if f.endswith(\".nii.gz\")]\n",
    "\n",
    "nifti_flattened = []\n",
    "for file in tqdm(nifti_files):\n",
    "    full_path = os.path.join(path, file)\n",
    "    img = nib.load(full_path)\n",
    "    nifti_flattened.append(transfrom_nifti_to_2d(img))\n",
    "\n",
    "print(f\"\\nTotal NIfTI files loaded: {len(nifti_flattened)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9367a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The interplay of prefrontal and sensorimotor cortices during inhibitory control of learned motor behavior\n",
      "Social cognition, behaviour and therapy adherence in frontal lobe epilepsy: a study combining neuroeconomic and neuropsychological methods\n",
      "Neurofunctional Signature of Hyperfamiliarity for Unknown Faces.\n",
      "Mind-wandering in Parkinson's disease hallucinations reflects primary visual and default network coupling.\n",
      "Corrigendum to “Fear Processing in Dental Phobia during Crossmodal Symptom Provocation: An fMRI Study”\n",
      "Structural and functional connectivity of the subthalamic nucleus during vocal emotion decoding.\n",
      "Uncovering a context-specific connectional fingerprint of human dorsal premotor cortex.\n",
      "Depressive mood in pre-dialytic chronic kidney disease: Statistical parametric mapping analysis of Tc-99m ECD brain SPECT\n",
      "The effect of stimulus context on pitch representations in the human auditory cortex.\n",
      "Modulation of neuronal activity after spinal cord stimulation for neuropathic pain; H(2)15O PET study.\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "\n",
    "numpy_text = aligned_text.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "query_vec = nifti_flattened[0]\n",
    "\n",
    "# make sure query_vec has a batch dim:\n",
    "q = query_vec.unsqueeze(0)          # shape (1, 28542)\n",
    "\n",
    "# run it *through the encoder only*:\n",
    "with torch.no_grad():\n",
    "    z = neuro_encoder.encoder(q)    # → shape (1, 384)\n",
    "\n",
    "z = z.squeeze(0)                    # → shape (384,)\n",
    "encoded_vec = z.cpu().numpy().astype(np.float16)\n",
    "\n",
    "# now both are (·,384) and (384,) and you can do:\n",
    "cos_sim = (\n",
    "    (numpy_text / np.linalg.norm(numpy_text, axis=1, keepdims=True)) @\n",
    "    (encoded_vec / np.linalg.norm(encoded_vec))\n",
    ")\n",
    "\n",
    "\n",
    "top_inds = np.argsort(cos_sim)[::-1][:top_k]\n",
    "\n",
    "for i in top_inds:\n",
    "    print(titles[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "256d8eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Social cognition, behaviour and therapy adherence in frontal lobe epilepsy: a study combining neuroeconomic and neuropsychological methods\n",
      "Mind-wandering in Parkinson's disease hallucinations reflects primary visual and default network coupling.\n",
      "An optimized voxel-based morphometric study of gray matter changes in patients with left-sided and right-sided mesial temporal lobe epilepsy and hippocampal sclerosis (MTLE/HS).\n",
      "Depressive mood in pre-dialytic chronic kidney disease: Statistical parametric mapping analysis of Tc-99m ECD brain SPECT\n",
      "Insular and Anterior Cingulate Circuits in Smokers with Schizophrenia\n",
      "Modulation of neuronal activity after spinal cord stimulation for neuropathic pain; H(2)15O PET study.\n",
      "A balancing act of the brain: activations and deactivations driven by cognitive load.\n",
      "Corrigendum to “Fear Processing in Dental Phobia during Crossmodal Symptom Provocation: An fMRI Study”\n",
      "When Action Observation Facilitates Visual Perception: Activation in Visuo-Motor  Areas Contributes to Object Recognition.\n",
      "Participation of visual association areas in social processing emerges when rTPJ is inhibited\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "\n",
    "numpy_text = aligned_text.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "query_vec = nifti_flattened[2]\n",
    "\n",
    "# make sure query_vec has a batch dim:\n",
    "q = query_vec.unsqueeze(0)          # shape (1, 28542)\n",
    "\n",
    "# run it *through the encoder only*:\n",
    "with torch.no_grad():\n",
    "    z = neuro_encoder.encoder(q)    # → shape (1, 384)\n",
    "\n",
    "z = z.squeeze(0)                    # → shape (384,)\n",
    "encoded_vec = z.cpu().numpy().astype(np.float16)\n",
    "\n",
    "# now both are (·,384) and (384,) and you can do:\n",
    "cos_sim = (\n",
    "    (numpy_text / np.linalg.norm(numpy_text, axis=1, keepdims=True)) @\n",
    "    (encoded_vec / np.linalg.norm(encoded_vec))\n",
    ")\n",
    "\n",
    "\n",
    "top_inds = np.argsort(cos_sim)[::-1][:top_k]\n",
    "\n",
    "for i in top_inds:\n",
    "    print(titles[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "819ffeb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>doi</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24911975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1371/journal.pone.0099222</td>\n",
       "      <td>Acute aerobic exercise increases cortical acti...</td>\n",
       "      <td>There is increasing evidence that acute aerobi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22884992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.dcn.2012.07.001</td>\n",
       "      <td>Developmental differences in the neural correl...</td>\n",
       "      <td>Despite vast knowledge on the behavioral proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15722210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.cogbrainres.2004.09.011</td>\n",
       "      <td>The neural substrate of arithmetic operations ...</td>\n",
       "      <td>Recent functional neuroimaging studies have be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21930137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.neuropsychologia.2011.09.006</td>\n",
       "      <td>Neural processing associated with comprehensio...</td>\n",
       "      <td>In daily communication, we often use indirect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21930160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1097/gme.0b013e3181cc49e9</td>\n",
       "      <td>Postmenopausal hormone use impact on emotion p...</td>\n",
       "      <td>Despite considerable evidence for potential ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28388</th>\n",
       "      <td>11923438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1523/JNEUROSCI.22-07-02730.2002</td>\n",
       "      <td>The neural correlates of moral sensitivity: a ...</td>\n",
       "      <td>Humans are endowed with a natural sense of fai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28389</th>\n",
       "      <td>12873805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/S0006-3223(02)01749-3</td>\n",
       "      <td>Abnormalities in emotion processing within cor...</td>\n",
       "      <td>BACKGROUND: Neurobiology of psychopathy is imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28390</th>\n",
       "      <td>19925196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1162/jocn.2009.21387</td>\n",
       "      <td>Virus and epidemic: causal knowledge activates...</td>\n",
       "      <td>Knowledge about cause and effect relationships...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28391</th>\n",
       "      <td>16001111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1007/s00213-005-0077-5</td>\n",
       "      <td>A functional MRI study of the effects of bromo...</td>\n",
       "      <td>RATIONALE: Dopamine is abundant in the prefron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28392</th>\n",
       "      <td>17524673</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.neuroimage.2007.03.062</td>\n",
       "      <td>Diffusion tensor MRI-based estimation of the i...</td>\n",
       "      <td>We evaluate and discuss the relevance of fiber...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28393 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           pmid  pmcid                                     doi  \\\n",
       "0      24911975    NaN            10.1371/journal.pone.0099222   \n",
       "1      22884992    NaN               10.1016/j.dcn.2012.07.001   \n",
       "2      15722210    NaN       10.1016/j.cogbrainres.2004.09.011   \n",
       "3      21930137    NaN  10.1016/j.neuropsychologia.2011.09.006   \n",
       "4      21930160    NaN            10.1097/gme.0b013e3181cc49e9   \n",
       "...         ...    ...                                     ...   \n",
       "28388  11923438    NaN      10.1523/JNEUROSCI.22-07-02730.2002   \n",
       "28389  12873805    NaN           10.1016/S0006-3223(02)01749-3   \n",
       "28390  19925196    NaN                 10.1162/jocn.2009.21387   \n",
       "28391  16001111    NaN               10.1007/s00213-005-0077-5   \n",
       "28392  17524673    NaN        10.1016/j.neuroimage.2007.03.062   \n",
       "\n",
       "                                                    name  \\\n",
       "0      Acute aerobic exercise increases cortical acti...   \n",
       "1      Developmental differences in the neural correl...   \n",
       "2      The neural substrate of arithmetic operations ...   \n",
       "3      Neural processing associated with comprehensio...   \n",
       "4      Postmenopausal hormone use impact on emotion p...   \n",
       "...                                                  ...   \n",
       "28388  The neural correlates of moral sensitivity: a ...   \n",
       "28389  Abnormalities in emotion processing within cor...   \n",
       "28390  Virus and epidemic: causal knowledge activates...   \n",
       "28391  A functional MRI study of the effects of bromo...   \n",
       "28392  Diffusion tensor MRI-based estimation of the i...   \n",
       "\n",
       "                                             description  \n",
       "0      There is increasing evidence that acute aerobi...  \n",
       "1      Despite vast knowledge on the behavioral proce...  \n",
       "2      Recent functional neuroimaging studies have be...  \n",
       "3      In daily communication, we often use indirect ...  \n",
       "4      Despite considerable evidence for potential ef...  \n",
       "...                                                  ...  \n",
       "28388  Humans are endowed with a natural sense of fai...  \n",
       "28389  BACKGROUND: Neurobiology of psychopathy is imp...  \n",
       "28390  Knowledge about cause and effect relationships...  \n",
       "28391  RATIONALE: Dopamine is abundant in the prefron...  \n",
       "28392  We evaluate and discuss the relevance of fiber...  \n",
       "\n",
       "[28393 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91f4e347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28757, 384])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_text.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c62b1c8",
   "metadata": {},
   "source": [
    "# Brain map to text. return top 10 papers with summarized abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4be1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "facebook_bart = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a24e26ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize(article):\n",
    "    \"\"\"\n",
    "    Summarizes an article using facebook/bart-large-cnn model\n",
    "    \n",
    "    Args:\n",
    "        article (str): Text to summarize\n",
    "        \n",
    "    Returns:\n",
    "        str: Summarized text\n",
    "    \"\"\"\n",
    "    dic = facebook_bart(article, max_length=256, min_length=50, do_sample=False)\n",
    "    summary_text = dict(dic[0])['summary_text']\n",
    "    return summary_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0f3098c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aligned_titles_half' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m top_k = \u001b[32m10\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m numpy_titles = \u001b[43maligned_titles_half\u001b[49m.detach().cpu().numpy().astype(np.float16)\n\u001b[32m      5\u001b[39m query_vec = nifti_flattened[\u001b[32m2\u001b[39m]\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# make sure query_vec has a batch dim:\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'aligned_titles_half' is not defined"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "\n",
    "numpy_titles = aligned_titles_half.detach().cpu().numpy().astype(np.float16)\n",
    "\n",
    "query_vec = nifti_flattened[2]\n",
    "\n",
    "# make sure query_vec has a batch dim:\n",
    "q = query_vec.unsqueeze(0)          # shape (1, 28542)\n",
    "\n",
    "# run it *through the encoder only*:\n",
    "with torch.no_grad():\n",
    "    z = neuro_encoder.encoder(q)    # → shape (1, 384)\n",
    "\n",
    "z = z.squeeze(0)                    # → shape (384,)\n",
    "encoded_vec = z.cpu().numpy().astype(np.float16)\n",
    "\n",
    "# now both are (·,384) and (384,) and you can do:\n",
    "cos_sim = (\n",
    "    (numpy_titles / np.linalg.norm(numpy_titles, axis=1, keepdims=True)) @\n",
    "    (encoded_vec / np.linalg.norm(encoded_vec))\n",
    ")\n",
    "\n",
    "\n",
    "top_inds = np.argsort(cos_sim)[::-1][:top_k]\n",
    "\n",
    "for i in top_inds:\n",
    "    print(titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b530b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
