{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93a801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting extraction ---\n",
      "\n",
      "\n",
      "‚úÖ Extraction complete.\n",
      "\n",
      "                          DOI          Source  Page   x   y   z\n",
      "0   10.1101/2024.11.20.624446       Paragraph     3 -45   0  45\n",
      "1   10.1101/2024.11.20.624446       Paragraph     3  42   6  57\n",
      "2   10.1101/2024.11.20.624446       Paragraph     3 -63   0  24\n",
      "3   10.1101/2024.11.20.624446       Paragraph     3  60  -3  24\n",
      "4   10.1101/2024.11.20.624446       Paragraph     3 -42  15  -3\n",
      "..                        ...             ...   ...  ..  ..  ..\n",
      "91  10.1101/2024.11.20.624446  Table (stream)    18  44  83 -51\n",
      "92  10.1101/2024.11.20.624446  Table (stream)    18  45  56 -45\n",
      "93  10.1101/2024.11.20.624446  Table (stream)    18  46  67 -30\n",
      "94  10.1101/2024.11.20.624446  Table (stream)    18 -39  21  -6\n",
      "95  10.1101/2024.11.20.624446  Table (stream)    18 -42  15  -3\n",
      "\n",
      "[96 rows x 6 columns]\n",
      "\n",
      "üìÅ Saved results to 'mni_coordinates_extracted.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz\n",
    "import camelot\n",
    "import pandas as pd\n",
    "import requests\n",
    "import contextlib\n",
    "import sys\n",
    "import cloudscraper\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def suppress_output():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout, old_stderr = sys.stdout, sys.stderr\n",
    "        sys.stdout, sys.stderr = devnull, devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout, sys.stderr = old_stdout, old_stderr\n",
    "\n",
    "def read_tables(file_path: str, page: int, flavor: str):\n",
    "    with suppress_output():\n",
    "        try:\n",
    "            return camelot.read_pdf(file_path, pages=str(page), flavor=flavor)\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "def is_valid_mni_triplet(x, y, z, bound=100):\n",
    "    return all(-bound < val < bound for val in (x, y, z))\n",
    "\n",
    "def extract_from_table(table_df: pd.DataFrame, validity_threshold=0.7):\n",
    "    mni_data, valid_rows, candidate_rows = [], 0, 0\n",
    "    for _, row in table_df.iterrows():\n",
    "        numeric = []\n",
    "        for cell in row.dropna().tolist():\n",
    "            try:\n",
    "                numeric.append(int(cell))\n",
    "            except:\n",
    "                continue\n",
    "        if len(numeric) >= 3:\n",
    "            candidate_rows += 1\n",
    "            for i in range(len(numeric) - 2):\n",
    "                x, y, z = numeric[i], numeric[i+1], numeric[i+2]\n",
    "                if is_valid_mni_triplet(x, y, z):\n",
    "                    valid_rows += 1\n",
    "                    mni_data.append((x, y, z))\n",
    "                    break\n",
    "    if candidate_rows and valid_rows / candidate_rows >= validity_threshold:\n",
    "        return mni_data\n",
    "    return []\n",
    "\n",
    "def extract_from_paragraphs(doc, keyword_window=80):\n",
    "    \"\"\"\"\"\n",
    "    For each regex match I tried to check nearby text for coordinate keywords.\n",
    "    \"\"\"\n",
    "    # Pattern: [x,y,z], (x,y,z), x, y, z, \"x, y and z\"\n",
    "    pattern = re.compile(\n",
    "        r'(?:(?<=\\[)|(?<=\\()|(?<![\\d\\w]))\\s*(-?\\d{1,3})\\s*(?:,|and| )+\\s*(-?\\d{1,3})\\s*(?:,|and| )+\\s*(-?\\d{1,3})(?=[\\]\\)]|[^0-9]|$)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    # Keywords\n",
    "    coord_keywords = re.compile(r'\\b(MNI|coordinate|coordinates|activation|cluster|peak|voxel|x, y, z)\\b', re.IGNORECASE)\n",
    "\n",
    "    para_results = []\n",
    "    for page_num in range(len(doc)):\n",
    "        text = doc[page_num].get_text(\"text\")\n",
    "        if not text or not coord_keywords.search(text):\n",
    "            continue\n",
    "\n",
    "        for m in pattern.finditer(text):\n",
    "            try:\n",
    "                x, y, z = map(int, m.groups())\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            if not is_valid_mni_triplet(x, y, z, bound=100):\n",
    "                continue\n",
    "\n",
    "            start = max(0, m.start() - keyword_window)\n",
    "            end = min(len(text), m.end() + keyword_window)\n",
    "            nearby = text[start:end]\n",
    "            if coord_keywords.search(nearby):\n",
    "                para_results.append({\n",
    "                    \"Source\": \"Paragraph\",\n",
    "                    \"Page\": page_num + 1,\n",
    "                    \"x\": x, \"y\": y, \"z\": z\n",
    "                })\n",
    "\n",
    "    return para_results\n",
    "\n",
    "# MAIN\n",
    "\n",
    "pdf_url = input(\"Enter PDF URL: \").strip()\n",
    "doi = input(\"Enter DOI: \").strip()\n",
    "file_path = \"paper_universal.pdf\"\n",
    "\n",
    "scraper = cloudscraper.create_scraper()\n",
    "response = scraper.get(pdf_url)\n",
    "with open(file_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(\"\\n--- Starting extraction ---\\n\")\n",
    "\n",
    "doc = fitz.open(file_path)\n",
    "total_pages = len(doc)\n",
    "results = []\n",
    "\n",
    "# 1) Table extraction\n",
    "table_coords_set = set() \n",
    "table_results = []\n",
    "\n",
    "for flavor in [\"stream\", \"lattice\"]:\n",
    "    for page in range(1, total_pages + 1):\n",
    "        tables = read_tables(file_path, page, flavor)\n",
    "        for table in tables:\n",
    "            coords = extract_from_table(table.df)\n",
    "            for (x, y, z) in coords:\n",
    "                entry = {\"Source\": f\"Table ({flavor})\", \"Page\": page, \"x\": x, \"y\": y, \"z\": z}\n",
    "                table_results.append(entry)\n",
    "                table_coords_set.add((page, x, y, z))\n",
    "\n",
    "results.extend(table_results)\n",
    "\n",
    "# 2) Paragraph extraction:\n",
    "para_results = extract_from_paragraphs(doc, keyword_window=80)\n",
    "for entry in para_results:\n",
    "    key = (entry[\"Page\"], entry[\"x\"], entry[\"y\"], entry[\"z\"])\n",
    "    if key in table_coords_set:\n",
    "        continue\n",
    "    results.append(entry)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "if not df.empty:\n",
    "    # remove duplicates\n",
    "    df = df.drop_duplicates(subset=[\"Page\", \"x\", \"y\", \"z\"], keep=\"first\")\n",
    "    df.insert(0, \"DOI\", doi)\n",
    "    df = df.sort_values(by=[\"Page\", \"Source\"])\n",
    "    print(\"\\nExtraction complete.\\n\")\n",
    "    print(df.reset_index(drop=True))\n",
    "    df.to_csv(\"mni_coordinates_extracted.csv\", index=False)\n",
    "    print(\"\\n Saved results to 'mni_coordinates_extracted.csv'\")\n",
    "else:\n",
    "    print(\"No coordinates found in this document.\")\n",
    "\n",
    "doc.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
