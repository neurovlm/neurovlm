{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce28585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from neurovlm.data import get_data_dir\n",
    "from neurovlm.models import TextAligner\n",
    "from neurovlm.loss import InfoNCELoss, recall_n, mix_match\n",
    "from neurovlm.train import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5317d0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeuroVLM embeddings\n",
    "neurovlm_dir = get_data_dir()\n",
    "latent_text_specter_neuro, pmids_neurovlm = torch.load(neurovlm_dir / \"latent_specter2_neuro.pt\", weights_only=False).values()\n",
    "latent_text_specter, pmids_neurovlm = torch.load(neurovlm_dir / \"latent_specter2_proxi.pt\", weights_only=False).values()\n",
    "\n",
    "latent_neuro = torch.load(neurovlm_dir / \"latent_neuro_nc.pt\")\n",
    "df = pd.read_parquet(neurovlm_dir / \"publications_more.parquet\")\n",
    "\n",
    "# Mask for neurocontext papers only\n",
    "pmids = np.load(neurovlm_dir / \"pmids_neurocontext.npy\")\n",
    "mask = np.array(df['pmid'].isin(pmids))\n",
    "df = df[mask]\n",
    "latent_text_specter = latent_text_specter[mask]\n",
    "latent_text_specter_neuro = latent_text_specter_neuro[mask]\n",
    "neuro_vectors = torch.load(neurovlm_dir / \"neuro_vectors.pt\")[mask]\n",
    "\n",
    "latent_neuro = latent_neuro.to(\"mps\")\n",
    "latent_text_specter = torch.column_stack((latent_text_specter, latent_text_specter_neuro)).to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c582d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "recall_fn = partial(recall_n, thresh=0.95, reduce_mean=True)\n",
    "recall_20_nv = np.zeros(10)   # recall@20\n",
    "recall_200_nv = np.zeros(10)  # recall@200\n",
    "mix_match_nv = np.zeros(10)\n",
    "\n",
    "# CV\n",
    "n_epochs_nv = 101\n",
    "val_size = 1000\n",
    "kfolds = KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "\n",
    "for i, (inds_train, inds_test) in enumerate(kfolds.split(np.arange(len(latent_neuro)))):\n",
    "\n",
    "    print(f\"Fold: {i}\")\n",
    "\n",
    "    # Data loaders and output directory\n",
    "    fold_dir = neurovlm_dir / \"models\"\n",
    "    fold_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    np.random.seed(i)\n",
    "    np.random.shuffle(inds_train)\n",
    "\n",
    "    inds_val = inds_train[:val_size]\n",
    "    inds_train = inds_train[val_size:]\n",
    "\n",
    "    # Projection head (align latent text to latent neuro)\n",
    "    proj_head_text = TextAligner(latent_text_dim=int(768*2), hidden_dim=768, seed=0).to('mps')\n",
    "    proj_head_neuro = TextAligner(latent_text_dim=768, hidden_dim=768, seed=1).to('mps')\n",
    "    optimizer = torch.optim.AdamW(chain(proj_head_neuro.parameters(), proj_head_text.parameters()), lr=5e-5)\n",
    "    loss_fn = InfoNCELoss(temperature=.1)\n",
    "    batch_size = int(2048 * 3)\n",
    "    best_loss = np.inf\n",
    "    inds_train_rand = inds_train.copy() # for random shuffling\n",
    "\n",
    "    # Train loop\n",
    "    for iepoch in range(n_epochs_nv):\n",
    "        np.random.shuffle(inds_train_rand)\n",
    "        for start_idx in range(0, len(inds_train), batch_size):\n",
    "\n",
    "            # Batch\n",
    "            end_idx = min(start_idx + batch_size, len(inds_train))\n",
    "            batch_inds = inds_train_rand[start_idx:end_idx]\n",
    "\n",
    "            # Forward pass\n",
    "            proj_neuro = proj_head_neuro(latent_neuro[batch_inds])\n",
    "            proj_text = proj_head_text(latent_text_specter[batch_inds])\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(proj_text, proj_neuro)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Validation loss\n",
    "            proj_neuro = proj_head_neuro(latent_neuro[inds_val])\n",
    "            proj_text = proj_head_text(latent_text_specter[inds_val])\n",
    "            loss = loss_fn(proj_text, proj_neuro)\n",
    "            if loss < best_loss:\n",
    "                best_loss = float(loss)\n",
    "                best_proj_neuro = deepcopy(proj_head_neuro)\n",
    "                best_proj_text = deepcopy(proj_head_text)\n",
    "\n",
    "            if iepoch  % 10 == 0:\n",
    "                # Report metrics occasionally\n",
    "                image_embeddings_nv = proj_head_neuro(latent_neuro[inds_val]).detach()\n",
    "                text_embeddings_nv = proj_head_text(latent_text_specter[inds_val]).detach()\n",
    "                image_embeddings_nv /= image_embeddings_nv.norm(dim=1)[:, None]\n",
    "                text_embeddings_nv /= text_embeddings_nv.norm(dim=1)[:, None]\n",
    "                similarity = (image_embeddings_nv @ text_embeddings_nv.T)\n",
    "                recall_20_nv_ = recall_fn(similarity.cpu().numpy(), np.eye(len(similarity)), n_first=20)\n",
    "                recall_200_nv_ = recall_fn(similarity.cpu().numpy(), np.eye(len(similarity)), n_first=200)\n",
    "                print(f\"Epoch: {iepoch}, Loss: {loss.item():.4f}, Recall@20: {recall_20_nv_:.4f}, Recall@200: {recall_200_nv_:.4f}\")\n",
    "\n",
    "    # Report test metrics for the current fold\n",
    "    image_embeddings_nv = proj_head_neuro(latent_neuro[inds_test]).detach()\n",
    "    text_embeddings_nv = proj_head_text(latent_text_specter[inds_test]).detach()\n",
    "    image_embeddings_nv /= image_embeddings_nv.norm(dim=1)[:, None]\n",
    "    text_embeddings_nv /= text_embeddings_nv.norm(dim=1)[:, None]\n",
    "    similarity = (image_embeddings_nv @ text_embeddings_nv.T).cpu().numpy()\n",
    "\n",
    "    recall_20_nv[i] = recall_fn(similarity, np.eye(len(similarity)), n_first=20)\n",
    "    recall_200_nv[i] = recall_fn(similarity, np.eye(len(similarity)), n_first=200)\n",
    "    mix_match_nv[i] = mix_match(similarity)\n",
    "    print(f\"Test Recall@20: {recall_20_nv[i]:.4f}, Recall@200: {recall_200_nv[i]:.4f}\")\n",
    "\n",
    "    torch.save(best_proj_neuro, fold_dir / f\"proj_neuro_fold{str(i).zfill(2)}.pt\")\n",
    "    torch.save(best_proj_text, fold_dir / f\"proj_text_fold{str(i).zfill(2)}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96406c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(neurovlm_dir / \"recall_20_nv_proxi_neuro.npy\", recall_20_nv)\n",
    "np.save(neurovlm_dir / \"recall_200_nv_proxi_neuro.npy\", recall_200_nv)\n",
    "np.save(neurovlm_dir / \"mix_match_nv_proxi_neuro.npy\", mix_match_nv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc4b8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f558610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuro\n",
    "# Test Recall@20: 0.1968, Recall@200: 0.5604\n",
    "# Test Recall@20: 0.2012, Recall@200: 0.5687\n",
    "# Test Recall@20: 0.1929, Recall@200: 0.5546\n",
    "# Test Recall@20: 0.2079, Recall@200: 0.5638\n",
    "# Test Recall@20: 0.1916, Recall@200: 0.5568\n",
    "# Test Recall@20: 0.1964, Recall@200: 0.5433\n",
    "# Test Recall@20: 0.2037, Recall@200: 0.5617\n",
    "# Test Recall@20: 0.2042, Recall@200: 0.5636\n",
    "# Test Recall@20: 0.2114, Recall@200: 0.5718\n",
    "# Test Recall@20: 0.1838, Recall@200: 0.5665\n",
    "\n",
    "# neuro + proxi\n",
    "# Test Recall@20: 0.2002, Recall@200: 0.5735\n",
    "# Test Recall@20: 0.2132, Recall@200: 0.5812\n",
    "# Test Recall@20: 0.1949, Recall@200: 0.5725\n",
    "# Test Recall@20: 0.2147, Recall@200: 0.5754\n",
    "# Test Recall@20: 0.2027, Recall@200: 0.5583\n",
    "# Test Recall@20: 0.2148, Recall@200: 0.5539\n",
    "# Test Recall@20: 0.2075, Recall@200: 0.5704\n",
    "# Test Recall@20: 0.2230, Recall@200: 0.5718\n",
    "# Test Recall@20: 0.2196, Recall@200: 0.5965\n",
    "# Test Recall@20: 0.2090, Recall@200: 0.5762\n",
    "\n",
    "# neuro + proxi + adhoc\n",
    "# Test Recall@20: 0.2132, Recall@200: 0.5759\n",
    "# Test Recall@20: 0.2113, Recall@200: 0.5837\n",
    "# Test Recall@20: 0.1958, Recall@200: 0.5638\n",
    "# Test Recall@20: 0.2152, Recall@200: 0.5856\n",
    "# Test Recall@20: 0.2027, Recall@200: 0.5709\n",
    "# Test Recall@20: 0.1945, Recall@200: 0.5588\n",
    "# Test Recall@20: 0.2080, Recall@200: 0.5728\n",
    "# Test Recall@20: 0.2172, Recall@200: 0.5767\n",
    "# Test Recall@20: 0.2221, Recall@200: 0.5849\n",
    "# Test Recall@20: 0.1911, Recall@200: 0.5830\n",
    "\n",
    "# Neurocontext\n",
    "# Test Loss: 0.2191, Recall@20: 0.5919, Recall@200: 0.5919\n",
    "# Test Loss: 0.2234, Recall@20: 0.5841, Recall@200: 0.5841\n",
    "# Test Loss: 0.2152, Recall@20: 0.5856, Recall@200: 0.5856\n",
    "# Test Loss: 0.2340, Recall@20: 0.5924, Recall@200: 0.5924\n",
    "# Test Loss: 0.2167, Recall@20: 0.5936, Recall@200: 0.5936\n",
    "# Test Loss: 0.2104, Recall@20: 0.5810, Recall@200: 0.5810\n",
    "# Test Loss: 0.2206, Recall@20: 0.5941, Recall@200: 0.5941\n",
    "# Test Loss: 0.2206, Recall@20: 0.5670, Recall@200: 0.5670\n",
    "# Test Loss: 0.2216, Recall@20: 0.6067, Recall@200: 0.6067\n",
    "# Test Loss: 0.1969, Recall@20: 0.5573, Recall@200: 0.5573"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c2b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0cbcbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeuroVLM embeddings\n",
    "neurovlm_dir = get_data_dir()\n",
    "latent_text_neuro, pmids_neurovlm = torch.load(neurovlm_dir / \"latent_specter2_neuro.pt\", weights_only=False).values()\n",
    "latent_text_adhoc, pmids_neurovlm = torch.load(neurovlm_dir / \"latent_specter2_adhoc.pt\", weights_only=False).values()\n",
    "latent_text_proxi, pmids_neurovlm = torch.load(neurovlm_dir / \"latent_specter2_proxi.pt\", weights_only=False).values()\n",
    "\n",
    "latent_neuro = torch.load(neurovlm_dir / \"latent_neuro_nc.pt\")\n",
    "df = pd.read_parquet(neurovlm_dir / \"publications_more.parquet\")\n",
    "\n",
    "# Mask for neurocontext papers only\n",
    "pmids = np.load(neurovlm_dir / \"pmids_neurocontext.npy\")\n",
    "mask = np.array(df['pmid'].isin(pmids))\n",
    "df = df[mask]\n",
    "latent_text_adhoc = latent_text_adhoc[mask]\n",
    "latent_text_neuro = latent_text_neuro[mask]\n",
    "latent_text_proxi = latent_text_proxi[mask]\n",
    "\n",
    "neuro_vectors = torch.load(neurovlm_dir / \"neuro_vectors.pt\")[mask]\n",
    "\n",
    "latent_neuro = latent_neuro.to(\"mps\")\n",
    "latent_text_specter = torch.column_stack((latent_text_adhoc, latent_text_neuro, latent_text_proxi)).to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a058b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "recall_fn = partial(recall_n, thresh=0.95, reduce_mean=True)\n",
    "recall_20_nv = np.zeros(10)   # recall@20\n",
    "recall_200_nv = np.zeros(10)  # recall@200\n",
    "mix_match_nv = np.zeros(10)\n",
    "\n",
    "# CV\n",
    "n_epochs_nv = 151\n",
    "val_size = 1000\n",
    "kfolds = KFold(n_splits=10, random_state=0, shuffle=True)\n",
    "\n",
    "for i, (inds_train, inds_test) in enumerate(kfolds.split(np.arange(len(latent_neuro)))):\n",
    "\n",
    "    print(f\"Fold: {i}\")\n",
    "\n",
    "    # Data loaders and output directory\n",
    "    fold_dir = neurovlm_dir / \"models\"\n",
    "    fold_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    np.random.seed(i)\n",
    "    np.random.shuffle(inds_train)\n",
    "\n",
    "    inds_val = inds_train[:val_size]\n",
    "    inds_train = inds_train[val_size:]\n",
    "\n",
    "    # Projection head (align latent text to latent neuro)\n",
    "    proj_head_text = TextAligner(latent_text_dim=int(768*3), hidden_dim=768, seed=0).to('mps')\n",
    "    proj_head_neuro = TextAligner(latent_text_dim=768, hidden_dim=768, seed=1).to('mps')\n",
    "    optimizer = torch.optim.AdamW(chain(proj_head_neuro.parameters(), proj_head_text.parameters()), lr=5e-5)\n",
    "    loss_fn = InfoNCELoss(temperature=.1)\n",
    "    batch_size = int(2048 * 3)\n",
    "    best_loss = np.inf\n",
    "    inds_train_rand = inds_train.copy() # for random shuffling\n",
    "\n",
    "    # Train loop\n",
    "    for iepoch in range(n_epochs_nv):\n",
    "        np.random.shuffle(inds_train_rand)\n",
    "        for start_idx in range(0, len(inds_train), batch_size):\n",
    "\n",
    "            # Batch\n",
    "            end_idx = min(start_idx + batch_size, len(inds_train))\n",
    "            batch_inds = inds_train_rand[start_idx:end_idx]\n",
    "\n",
    "            # Forward pass\n",
    "            proj_neuro = proj_head_neuro(latent_neuro[batch_inds])\n",
    "            proj_text = proj_head_text(latent_text_specter[batch_inds])\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(proj_text, proj_neuro)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Validation loss\n",
    "            proj_neuro = proj_head_neuro(latent_neuro[inds_val])\n",
    "            proj_text = proj_head_text(latent_text_specter[inds_val])\n",
    "            loss = loss_fn(proj_text, proj_neuro)\n",
    "            if loss < best_loss:\n",
    "                best_loss = float(loss)\n",
    "                best_proj_neuro = deepcopy(proj_head_neuro)\n",
    "                best_proj_text = deepcopy(proj_head_text)\n",
    "\n",
    "            if iepoch  % 10 == 0:\n",
    "                # Report metrics occasionally\n",
    "                image_embeddings_nv = proj_head_neuro(latent_neuro[inds_val]).detach()\n",
    "                text_embeddings_nv = proj_head_text(latent_text_specter[inds_val]).detach()\n",
    "                image_embeddings_nv /= image_embeddings_nv.norm(dim=1)[:, None]\n",
    "                text_embeddings_nv /= text_embeddings_nv.norm(dim=1)[:, None]\n",
    "                similarity = (image_embeddings_nv @ text_embeddings_nv.T)\n",
    "                recall_20_nv_ = recall_fn(similarity.cpu().numpy(), np.eye(len(similarity)), n_first=20)\n",
    "                recall_200_nv_ = recall_fn(similarity.cpu().numpy(), np.eye(len(similarity)), n_first=200)\n",
    "                print(f\"Epoch: {iepoch}, Loss: {loss.item():.4f}, Recall@20: {recall_20_nv_:.4f}, Recall@200: {recall_200_nv_:.4f}\")\n",
    "\n",
    "    # Report test metrics for the current fold\n",
    "    image_embeddings_nv = proj_head_neuro(latent_neuro[inds_test]).detach()\n",
    "    text_embeddings_nv = proj_head_text(latent_text_specter[inds_test]).detach()\n",
    "    image_embeddings_nv /= image_embeddings_nv.norm(dim=1)[:, None]\n",
    "    text_embeddings_nv /= text_embeddings_nv.norm(dim=1)[:, None]\n",
    "    similarity = (image_embeddings_nv @ text_embeddings_nv.T).cpu().numpy()\n",
    "\n",
    "    recall_20_nv[i] = recall_fn(similarity, np.eye(len(similarity)), n_first=20)\n",
    "    recall_200_nv[i] = recall_fn(similarity, np.eye(len(similarity)), n_first=200)\n",
    "    mix_match_nv[i] = mix_match(similarity)\n",
    "    print(f\"Test Recall@20: {recall_20_nv[i]:.4f}, Recall@200: {recall_200_nv[i]:.4f}\")\n",
    "\n",
    "    torch.save(best_proj_neuro, fold_dir / f\"proj_neuro_fold{str(i).zfill(2)}.pt\")\n",
    "    torch.save(best_proj_text, fold_dir / f\"proj_text_fold{str(i).zfill(2)}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0ff624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(neurovlm_dir / \"recall_20_nv_proxi_adhoc_neuro.npy\", recall_20_nv)\n",
    "np.save(neurovlm_dir / \"recall_200_nv_proxi_adhoc_neuro.npy\", recall_200_nv)\n",
    "np.save(neurovlm_dir / \"mix_match_nv_proxi_adhoc_neuro.npy\", mix_match_nv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d373ced4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
