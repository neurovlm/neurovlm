{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4863507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from neurovlm.loss import InfoNCELoss\n",
    "from neurovlm.data import data_dir\n",
    "from neurovlm.train import Trainer, which_device\n",
    "from neurovlm.models import TextAligner\n",
    "from neurovlm.loss import InfoNCELoss\n",
    "from neurovlm.retrieval_resources import _proj_head_mse_adhoc\n",
    "proj_head = _proj_head_mse_adhoc()\n",
    "device = which_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b2c129",
   "metadata": {},
   "source": [
    "# Projection Head\n",
    "\n",
    "Projection head refers to a small network to align the latent spaces between text and neuroimages. This notebook learns a project head to map the image and neuro latent spaces to one another using a contrastive loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b3d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoded neurovectors from the second notebook\n",
    "latent_neuro, pmids_latent = torch.load(data_dir / \"latent_neuro_sparse.pt\", weights_only=False).values()\n",
    "# latent_neuro = latent_neuro / latent_neuro.norm(dim=1)[:, None]\n",
    "\n",
    "# Load encoded text from last notebook\n",
    "latent_text_specter, pmids = torch.load(data_dir / \"latent_specter2_adhoc.pt\", weights_only=False).values()\n",
    "inds = np.argsort(pmids)\n",
    "latent_text_specter, pmids = latent_text_specter[inds], pmids[inds]\n",
    "\n",
    "mask =  pd.Series(pmids).isin(pmids_latent)\n",
    "pmids = pmids[mask]\n",
    "latent_text_specter = latent_text_specter[mask]\n",
    "assert (pmids == pmids_latent).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacccf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = latent_neuro.norm(dim=1).detach().cpu().numpy() < 35 # sparser targets\n",
    "# pmids = pmids[mask]\n",
    "# latent_neuro = latent_neuro[mask]\n",
    "# latent_text_specter = latent_text_specter[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13c0023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load splits\n",
    "ids_train, ids_test, ids_val = torch.load(data_dir / \"pmids_split.pt\", weights_only=False).values()\n",
    "train_inds = np.where(pd.Series(pmids).isin(ids_train))[0]\n",
    "test_inds = np.where(pd.Series(pmids).isin(ids_test))[0]\n",
    "val_inds = np.where(pd.Series(pmids).isin(ids_val))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e49ab3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5457e837372f4b04aaf3b71091ddaf89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, val loss: 7.8383\n",
      "Epoch: 10, val loss: 7.5797\n",
      "Epoch: 20, val loss: 7.4161\n",
      "Epoch: 30, val loss: 7.3353\n",
      "Epoch: 40, val loss: 7.2921\n",
      "Epoch: 50, val loss: 7.2657\n",
      "Epoch: 60, val loss: 7.2477\n",
      "Epoch: 70, val loss: 7.2343\n",
      "Epoch: 80, val loss: 7.2234\n",
      "Epoch: 90, val loss: 7.2146\n",
      "Epoch: 100, val loss: 7.207\n",
      "Epoch: 110, val loss: 7.2005\n",
      "Epoch: 120, val loss: 7.1945\n",
      "Epoch: 130, val loss: 7.1893\n",
      "Epoch: 140, val loss: 7.1843\n",
      "Epoch: 150, val loss: 7.1801\n",
      "Epoch: 160, val loss: 7.1761\n",
      "Epoch: 170, val loss: 7.1724\n",
      "Epoch: 180, val loss: 7.169\n",
      "Epoch: 190, val loss: 7.1657\n",
      "Epoch: 199, val loss: 7.1633\n"
     ]
    }
   ],
   "source": [
    "proj_head = _proj_head_mse_adhoc()\n",
    "\n",
    "# Split data\n",
    "X_train_image = latent_neuro[train_inds].to(device)\n",
    "X_train_text  = latent_text_specter[train_inds].to(device)\n",
    "X_val_image = latent_neuro[val_inds].to(device)\n",
    "X_val_text  = latent_text_specter[val_inds].to(device)\n",
    "\n",
    "# Models\n",
    "proj_head_text  = proj_head.to(device) # initialize with the decoder model\n",
    "# proj_head_text  = TextAligner(seed=123, latent_text_dim=768, hidden_dim=512, latent_neuro_dim=384).to(device)\n",
    "proj_head_image = TextAligner(seed=123, latent_text_dim=384, hidden_dim=384, latent_neuro_dim=384).to(device)\n",
    "\n",
    "# Settings\n",
    "loss_fn = InfoNCELoss(temperature=0.2)\n",
    "n_epochs = 200\n",
    "batch_size = 2048#512\n",
    "lr = 1e-5\n",
    "optimizer = AdamW([*proj_head_text.parameters(), *proj_head_image.parameters()], lr=lr)\n",
    "interval = 10\n",
    "\n",
    "# Train\n",
    "iterable = tqdm(range(n_epochs), total=n_epochs)\n",
    "\n",
    "for iepoch in iterable:\n",
    "\n",
    "    proj_head_text.train()\n",
    "    proj_head_image.train()\n",
    "\n",
    "    # Randomly shuffle and batch\n",
    "    torch.manual_seed(iepoch)\n",
    "    rand_inds = torch.randperm(len(X_train_image), device=device)\n",
    "\n",
    "    for i in range(0, len(X_train_image), batch_size):\n",
    "        idx = rand_inds[i:i+batch_size]\n",
    "\n",
    "        # Forward\n",
    "        y_text  = proj_head_text(X_train_text[idx])\n",
    "        y_image = proj_head_image(X_train_image[idx])\n",
    "\n",
    "        # Loss\n",
    "        loss = loss_fn(y_text, y_image)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "\n",
    "        # Step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Report validation\n",
    "    if iepoch % interval == 0 or iepoch == (n_epochs - 1):\n",
    "        proj_head_text.eval()\n",
    "        proj_head_image.eval()\n",
    "        with torch.no_grad():\n",
    "            y_text  = proj_head_text(X_val_text)\n",
    "            y_image = proj_head_image(X_val_image)\n",
    "            val_loss = loss_fn(y_text, y_image)\n",
    "            print(f\"Epoch: {iepoch}, val loss: {float(val_loss):.5g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57e6dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(proj_head_text, data_dir / \"proj_head_text_infonce.pt\")\n",
    "torch.save(proj_head_image, data_dir / \"proj_head_image_infonce.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
