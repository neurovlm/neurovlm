{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bba59bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import ollama\n",
    "from ollama import chat\n",
    "\n",
    "import nibabel as nib\n",
    "from nilearn import maskers\n",
    "from nilearn.plotting import view_img\n",
    "from nilearn.image import resample_img\n",
    "\n",
    "from neurovlm.data import get_data_dir\n",
    "from neurovlm.models import Specter\n",
    "from neurovlm.text_input import search_papers_from_text, search_wiki_from_text, generate_llm_response_from_text\n",
    "from neurovlm.brain_input import search_papers_from_brain, search_wiki_from_brain, generate_llm_response_from_brain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71187e37",
   "metadata": {},
   "source": [
    "## Ollama Setup\n",
    "\n",
    "**1. Install Ollama**\n",
    "\n",
    "- **macOS:**  \n",
    "    ```sh\n",
    "    curl -fsSL https://ollama.com/install.sh | sh\n",
    "    ```\n",
    "- **Linux:**  \n",
    "    ```sh\n",
    "    curl -fsSL https://ollama.com/install.sh | sh\n",
    "    ```\n",
    "\n",
    "**2. Start Ollama Service**\n",
    "    ```\n",
    "    ollama serve\n",
    "    ```\n",
    "    \n",
    "**3. Pull a Model (Our default model is qwen2.5:3b-instruct)**\n",
    "    ```\n",
    "    ollama pull qwen2.5:3b-instruct\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0b30c7",
   "metadata": {},
   "source": [
    "## Load data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f5edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and Specter\n",
    "data_dir = get_data_dir()\n",
    "autoencoder = torch.load(data_dir / \"autoencoder_sparse.pt\", weights_only=False).cpu()\n",
    "\n",
    "wiki_df = pd.read_parquet(f\"{data_dir}/neurowiki_with_ids.parquet\")\n",
    "\n",
    "\n",
    "mask_arrays = np.load(f\"{data_dir}/mask.npz\", allow_pickle=True)\n",
    "mask_img = nib.Nifti1Image(mask_arrays[\"mask\"].astype(float),  mask_arrays[\"affine\"])\n",
    "masker = maskers.NiftiMasker(mask_img=mask_img, dtype=np.float32).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2549c56",
   "metadata": {},
   "source": [
    "## Example query for text\n",
    "\n",
    "The titles and abstract most related to the query will be passed to the LM.\n",
    "\n",
    "The functions expect a string input; no need to encode the input text before passing it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3329e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"default mode network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7de5ac50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default mode network'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f2e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There are adapters available but none are activated for the forward pass.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top matches:\n",
      "1. Spatio-temporal brain dynamics of self-identity: an EEG source analysis of the current and past self\n",
      "2. The attribution of animacy and agency in frontotemporal dementia versus Alzheimer's disease.\n",
      "3. Tracking the dynamic functional connectivity structure of the human brain across the adult lifespan.\n",
      "4. Finding the vanished self: Perspective modulates neural substrates of self-reflection in Buddhists.\n",
      "5. Intrinsic Connectivity Identifies the Hippocampus as a Main Crossroad between Alzheimerâ€™s and Semantic Dementia-Targeted Networks\n"
     ]
    }
   ],
   "source": [
    "# Get the top k similar papers\n",
    "abstract, titles = search_papers_from_text(query, show_titles = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0377d8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top matches:\n",
      "1. Hallmarks of aging\n",
      "2. Beta wave\n",
      "3. Biological organisation\n",
      "4. Low-density lipoprotein receptor-related protein 8\n",
      "5. Leon Cooper\n",
      "6. Outline of death\n",
      "7. Gerontology\n",
      "8. Biological system\n",
      "9. Theta model\n",
      "10. Postmortem caloricity\n"
     ]
    }
   ],
   "source": [
    "context, titles = search_wiki_from_text(\"dementia\", top_k=10, show_titles = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44087e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate_llm_response_from_text(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ff6716",
   "metadata": {},
   "source": [
    "## Example query for Brian Input\n",
    "\n",
    "The titles and abstract most related to the brain will be passed to the LM.\n",
    "\n",
    "The functions expects an already encoded brain input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0127f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network atlases\n",
    "with gzip.open(get_data_dir() / f\"networks_arrays.pkl.gz\", \"rb\") as f:\n",
    "    networks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9038320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = {k: v for _k in networks.keys() for k, v in networks[_k].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bf6c259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288047065eb14856be8db8341d2277d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "networks_resampled = {}\n",
    "\n",
    "for k in tqdm(networks.keys(), total=len(networks)):\n",
    "    img = nib.Nifti1Image(networks[k][\"array\"], affine=networks[k][\"affine\"])\n",
    "\n",
    "    if len(np.unique(networks[k][\"array\"])) == 2:\n",
    "        # binary data\n",
    "        img_resampled = resample_img(img, mask_arrays[\"affine\"], interpolation=\"nearest\")\n",
    "    else:\n",
    "        img_resampled = resample_img(img, mask_arrays[\"affine\"])\n",
    "        img_resampled_arr = img_resampled.get_fdata()\n",
    "        img_resampled_arr[img_resampled_arr < 0] = 0.\n",
    "        thresh = np.percentile(img_resampled_arr.flatten(), 95)\n",
    "        img_resampled_arr[img_resampled_arr < thresh] = 0.\n",
    "        img_resampled_arr[img_resampled_arr >= thresh] = 1.\n",
    "        img_resampled = nib.Nifti1Image(img_resampled_arr, affine=mask_arrays[\"affine\"])\n",
    "\n",
    "    networks_resampled[k] = img_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13db1a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65f691a08374d7b9ae134ee2438aaa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "networks_embed = {}\n",
    "\n",
    "for k, v in tqdm(networks_resampled.items(), total=len(networks_resampled)):\n",
    "    networks_embed[k] = autoencoder.encoder(torch.from_numpy(masker.transform(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fca2f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top matches:\n",
      "1. Neural correlates of sound externalization.\n",
      "2. Audio-visual synchrony modulates the ventriloquist illusion and its neural/spatial representation in the auditory cortex.\n",
      "3. Processing of spectral and amplitude envelope of animal vocalizations in the human auditory cortex.\n",
      "4. Music listening engages specific cortical regions within the temporal lobes: Differences between musicians and non-musicians.\n",
      "5. Processing of natural sounds in human auditory cortex: tonotopy, spectral tuning, and relation to voice sensitivity.\n"
     ]
    }
   ],
   "source": [
    "# Look for abstract and titles related to a Auditory network\n",
    "abstract, titles = search_papers_from_brain(networks_embed[\"AUD\"], show_titles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f62bdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top matches:\n",
      "1. Neurocomputational speech processing\n",
      "2. Speech segmentation\n",
      "3. Aphonia\n",
      "4. 2,N,N-TMT\n",
      "5. Whispering\n",
      "6. Misophonia\n",
      "7. Aphonogelia\n",
      "8. Speech error\n",
      "9. Universal neonatal hearing screening\n",
      "10. Hypoglossal nerve stimulation\n"
     ]
    }
   ],
   "source": [
    "summary, titles = search_wiki_from_brain(networks_embed[\"AUD\"], top_k=10, show_titles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401467fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM summary for related abstracts\n",
    "generate_llm_response_from_brain(networks_embed[\"AUD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd4731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (neurovlm)",
   "language": "python",
   "name": "neurovlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
