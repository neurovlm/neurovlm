{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pickle, gzip\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import nibabel as nib\n",
    "from nilearn import maskers\n",
    "from nilearn.plotting import view_img\n",
    "from nilearn.image import resample_img\n",
    "\n",
    "from neurovlm.data import data_dir\n",
    "from neurovlm.models import Specter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab7d04f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network atlases\n",
    "with gzip.open(\"/Users/ryanhammonds/projects/cbig_network_correspondence/networks.pkl.gz\", \"rb\") as f:\n",
    "    networks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "de7ced54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There are adapters available but none are activated for the forward pass.\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "proj_head = torch.load(data_dir / f\"proj_head_mse_sparse.pt\", weights_only=False).cpu()\n",
    "specter = Specter(\"allenai/specter2_aug2023refresh\", adapter=\"adhoc_query\")\n",
    "autoencoder = torch.load(data_dir / \"autoencoder_sparse.pt\", weights_only=False).cpu()\n",
    "# decoder = autoencoder.decoder.to(\"cpu\")\n",
    "mask_arrays = np.load(f\"{data_dir}/mask.npz\", allow_pickle=True)\n",
    "mask_img = nib.Nifti1Image(mask_arrays[\"mask\"].astype(float),  mask_arrays[\"affine\"])\n",
    "masker = maskers.NiftiMasker(mask_img=mask_img, dtype=np.float32).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dd82076",
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = {k: v for _k in networks.keys() for k, v in networks[_k].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1745c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54ab26746f64d9a80065f09b6ef53a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "networks_resampled = {}\n",
    "\n",
    "for k, img in tqdm(networks.items(), total=len(networks)):\n",
    "    img_arr = img.get_fdata()\n",
    "\n",
    "    if len(np.unique(img_arr)) == 2:\n",
    "        # binary data\n",
    "        img_resampled = resample_img(img, mask_arrays[\"affine\"], interpolation=\"nearest\")\n",
    "    else:\n",
    "        img_resampled = resample_img(img, mask_arrays[\"affine\"])\n",
    "        img_resampled_arr = img_resampled.get_fdata()\n",
    "        img_resampled_arr[img_resampled_arr < 0] = 0.\n",
    "        thresh = np.percentile(img_resampled_arr.flatten(), 95)\n",
    "        img_resampled_arr[img_resampled_arr < thresh] = 0.\n",
    "        img_resampled_arr[img_resampled_arr >= thresh] = 1.\n",
    "        img_resampled = nib.Nifti1Image(img_resampled_arr, affine=mask_arrays[\"affine\"])\n",
    "\n",
    "    networks_resampled[k] = img_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7675016b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27cbb5dd850462cb63fb6c52715fbda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "networks_embed = {}\n",
    "\n",
    "for k, v in tqdm(networks_resampled.items(), total=len(networks_resampled)):\n",
    "    networks_embed[k] = autoencoder.encoder(torch.from_numpy(masker.transform(v)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1997029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_labels = [\n",
    "    'default mode network',\n",
    "    'frontoparietal network',\n",
    "    \"control network\",\n",
    "    'cognitive control network',\n",
    "    'dorsal attention network',\n",
    "    'salience network attention',\n",
    "    'somatosensory network',\n",
    "    'motor network',\n",
    "    'somatomotor network',\n",
    "    'visual network',\n",
    "    'language network',\n",
    "    'executive control network',\n",
    "    'auditory network',\n",
    "    'limbic emotional network',\n",
    "    'multiple demand network',\n",
    "    'subcortical network',\n",
    "    'cerebellar network'\n",
    "]\n",
    "\n",
    "label_embeddings = specter(network_labels)\n",
    "label_embeddings = label_embeddings / label_embeddings.norm(dim=1)[:, None]\n",
    "\n",
    "label_embeddings = proj_head(label_embeddings)\n",
    "label_embeddings = label_embeddings / label_embeddings.norm(dim=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fc74d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_label = []\n",
    "secondary_label = []\n",
    "primary_label_sim_score = []\n",
    "secondary_label_sim_score = []\n",
    "atlas_label = []\n",
    "results = {}\n",
    "i = 0\n",
    "for k, embed in networks_embed.items():\n",
    "    embed = embed / embed.norm()\n",
    "    cossim = label_embeddings @ embed\n",
    "    inds = torch.argsort(cossim, descending=True)\n",
    "    primary_label.append(network_labels[inds[0]])\n",
    "    primary_label_sim_score.append(float(cossim[inds[0]]))\n",
    "    secondary_label.append(network_labels[inds[1]])\n",
    "    secondary_label_sim_score.append(float(cossim[inds[1]]))\n",
    "    atlas_label.append(k)\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8176d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"atlas_label\": atlas_label,\n",
    "    \"predicted_label_primary\": primary_label,\n",
    "    \"predicted_label_secondary\": secondary_label,\n",
    "    \"primary_cossim_score\": primary_label_sim_score,\n",
    "    \"secondar_cossim_score\": secondary_label_sim_score,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7d9fbcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"~/Desktop/sim.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
